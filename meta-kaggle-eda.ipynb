{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "260f9c49",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-08-20T07:31:52.873318Z",
     "iopub.status.busy": "2023-08-20T07:31:52.872436Z",
     "iopub.status.idle": "2023-08-20T07:31:52.884179Z",
     "shell.execute_reply": "2023-08-20T07:31:52.883245Z"
    },
    "papermill": {
     "duration": 0.023883,
     "end_time": "2023-08-20T07:31:52.886704",
     "exception": false,
     "start_time": "2023-08-20T07:31:52.862821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c7b909",
   "metadata": {
    "papermill": {
     "duration": 0.007186,
     "end_time": "2023-08-20T07:31:52.901473",
     "exception": false,
     "start_time": "2023-08-20T07:31:52.894287",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Exploring folders and Subfolders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1e2af32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-20T07:31:52.918477Z",
     "iopub.status.busy": "2023-08-20T07:31:52.917823Z",
     "iopub.status.idle": "2023-08-20T07:31:54.042370Z",
     "shell.execute_reply": "2023-08-20T07:31:54.041056Z"
    },
    "papermill": {
     "duration": 1.136284,
     "end_time": "2023-08-20T07:31:54.045200",
     "exception": false,
     "start_time": "2023-08-20T07:31:52.908916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 644\r\n",
      "-rw-r--r-- 1 nobody nogroup  23653 Aug 12 01:07 12006057.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup  57159 Aug 12 01:07 12006089.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup 104291 Aug 12 01:07 12006102.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup   7237 Aug 12 01:07 12006114.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup  12785 Aug 12 01:07 12006141.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup   7018 Aug 12 01:07 12006176.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup  20022 Aug 12 01:07 12006178.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup   7018 Aug 12 01:07 12006301.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup   3376 Aug 12 01:07 12006336.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup  84528 Aug 12 01:07 12006361.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup  16979 Aug 12 01:07 12006396.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup  39880 Aug 12 01:07 12006424.py\r\n",
      "-rw-r--r-- 1 nobody nogroup   7018 Aug 12 01:07 12006458.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup   8439 Aug 12 01:07 12006486.py\r\n",
      "-rw-r--r-- 1 nobody nogroup   9580 Aug 12 01:07 12006502.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup   8532 Aug 12 01:07 12006505.py\r\n",
      "-rw-r--r-- 1 nobody nogroup  13658 Aug 12 01:07 12006559.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup   7018 Aug 12 01:07 12006599.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup  44756 Aug 12 01:07 12006620.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup  10039 Aug 12 01:07 12006699.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup  23645 Aug 12 01:07 12006714.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup   1758 Aug 12 01:07 12006763.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup   1758 Aug 12 01:07 12006815.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup   7018 Aug 12 01:07 12006830.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup  24469 Aug 12 01:07 12006874.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup   2182 Aug 12 01:07 12006890.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup   7018 Aug 12 01:07 12006930.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup   6978 Aug 12 01:07 12006933.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup  14972 Aug 12 01:07 12006952.ipynb\r\n",
      "-rw-r--r-- 1 nobody nogroup   2967 Aug 12 01:07 12006963.r\r\n",
      "-rw-r--r-- 1 nobody nogroup  23374 Aug 12 01:07 12006979.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ../input/meta-kaggle-code/0012/006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1be91730",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-20T07:31:54.063546Z",
     "iopub.status.busy": "2023-08-20T07:31:54.062164Z",
     "iopub.status.idle": "2023-08-20T07:31:55.174036Z",
     "shell.execute_reply": "2023-08-20T07:31:55.172182Z"
    },
    "papermill": {
     "duration": 1.12434,
     "end_time": "2023-08-20T07:31:55.177288",
     "exception": false,
     "start_time": "2023-08-20T07:31:54.052948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"cells\":[{\"metadata\":{\"_uuid\":\"8f2839f25d086af736a60e9eeb907d3b93b6e0e5\",\"_cell_guid\":\"b1076dfc-b9ad-4769-8c92-a6c4dae69d19\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n# For example, here's several helpful packages to load in \\n\\nimport numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\n\\n# Input data files are available in the \\\"../input/\\\" directory.\\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\\n\\nimport os\\nprint(os.listdir(\\\"../input\\\"))\\n\\n# Any results you write to the current directory are saved as output.\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"96413d12d4ca227ace5c9c07766f4f317f89511b\"},\"cell_type\":\"code\",\"source\":\"from sklearn.decomposition import KernelPCA\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.utils import class_weight\\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\\nfrom sklearn.model_selection import train_test_split\\nimport pickle\\nfrom keras.layers import Dense, Activation, Dropout, BatchNormalization, Input\\nfrom keras.models import Sequential, Model\\nfrom keras import optimizers, regularizers, initializers\\nfrom keras.callbacks import ModelCheckpoint, Callback\\nfrom keras import backend as K\\nfrom keras.optimizers import Adam\\nimport tensorflow as tf\\nfrom xgboost import XGBClassifier\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"a317a8478e0802bd232504777e5b5f970e3a788a\"},\"cell_type\":\"code\",\"source\":\"NCA1 = 50\\nNCA2 = 100\\nDROPRATE = 0.2\\nEP = 500\\nBATCH_SIZE = 256\\nVAL_RATIO = 0.1\\nTEST_RATIO = 0.1\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0917ba0d9889194b44548e8e1d8f935f83b8c9eb\"},\"cell_type\":\"markdown\",\"source\":\"## Loading BBBP dataset\\n\\n**BBBP**: Binary labels of blood-brain barrier penetration(permeability).\\n\"},{\"metadata\":{\"_cell_guid\":\"79c7e3d0-c299-4dcb-8224-4455121ee9b0\",\"_uuid\":\"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"bbbp_df= pd.read_csv('../input/bbbp_descriptors/BBBP_df_revised.csv')\\nprint(bbbp_df.shape)\\nbbbp_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"352c491782c8b62ce6904b5f3c85d0222805f34d\"},\"cell_type\":\"markdown\",\"source\":\"The simplified molecular-input line-entry system (**SMILES**) is a specification in form of a line notation for describing the structure of chemical species using short ASCII strings. SMILES can be converted to molecular structure by using RDKIT module.\\n\\nExample: \\n```python\\nfrom rdkit import Chem\\nm = Chem.MolFromSmiles('Cc1ccccc1')\\n```\\n\\nFurther reading:\\n* https://www.rdkit.org/docs/GettingStartedInPython.html\"},{\"metadata\":{\"trusted\":true,\"_uuid\":\"9fdb5b794648cc805159594527b6aec7abe5b494\"},\"cell_type\":\"code\",\"source\":\"bbbp_df['p_np'].value_counts()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"33af3e6f48775ebb81d3e34fd19a82955b58a64a\"},\"cell_type\":\"markdown\",\"source\":\"## Loading molecular descriptors\\n\\nDescriptors dataframe contains 1625 molecular descriptors (including 3D descriptors) generated on the NCI database using Mordred python module.\\n\\nFurther Reading:\\n* https://en.wikipedia.org/wiki/Molecular_descriptor\\n* https://github.com/mordred-descriptor/mordred\"},{\"metadata\":{\"trusted\":true,\"_uuid\":\"988b649adfb69cfc43f0d8d7f084608b6e845842\"},\"cell_type\":\"code\",\"source\":\"bbbp_descriptors_df= pd.read_csv('../input/bbbp_descriptors/BBBP_descriptors_df.csv',low_memory=False)\\nprint(bbbp_descriptors_df.shape)\\nbbbp_descriptors_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"scrolled\":true,\"_uuid\":\"9ddb2cfb56587b305160a9f8f4d15413915f7773\"},\"cell_type\":\"code\",\"source\":\"# function to coerce all data types to numeric\\n\\ndef coerce_to_numeric(df, column_list):\\n    df[column_list] = df[column_list].apply(pd.to_numeric, errors='coerce')\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"33e25f00d71457264a433a8301ec1d44b1aa235e\"},\"cell_type\":\"code\",\"source\":\"coerce_to_numeric(bbbp_descriptors_df, bbbp_descriptors_df.columns)\\nbbbp_descriptors_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"6d300a694864c8caaf83547b50f112ec7b49ed98\"},\"cell_type\":\"code\",\"source\":\"bbbp_descriptors_df = bbbp_descriptors_df.fillna(0)\\nbbbp_descriptors_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"157cd38347003f810b36860da9785cbbac338db1\"},\"cell_type\":\"markdown\",\"source\":\"## Scaling and Principal component analysis (PCA) \"},{\"metadata\":{\"trusted\":true,\"_uuid\":\"847505550ef7051fb8a3a8a2ed73e7be5149372b\"},\"cell_type\":\"code\",\"source\":\"bbbp_scaler1 = StandardScaler()\\nbbbp_scaler1.fit(bbbp_descriptors_df.values)\\nbbbp_descriptors_df = pd.DataFrame(bbbp_scaler1.transform(bbbp_descriptors_df.values),\\n                                   columns=bbbp_descriptors_df.columns)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"559be8883fed83ca7441c4f0ee6140772e4a8149\"},\"cell_type\":\"code\",\"source\":\"nca = NCA1\\ncn = ['col'+str(x) for x in range(nca)]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"91591e8dde4e8f8b44a8522973f071b1f23e4025\"},\"cell_type\":\"code\",\"source\":\"bbbp_transformer1 = KernelPCA(n_components=nca, kernel='rbf', n_jobs=-1)\\nbbbp_transformer1.fit(bbbp_descriptors_df.values)\\nbbbp_descriptors_df = pd.DataFrame(bbbp_transformer1.transform(bbbp_descriptors_df.values),\\n                                   columns=cn)\\nprint(bbbp_descriptors_df.shape)\\nbbbp_descriptors_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"d47f93361712ec31b371ac69c4e4e682c89bc899\"},\"cell_type\":\"code\",\"source\":\"X_train, X_test, y_train, y_test = train_test_split(bbbp_descriptors_df.values, bbbp_df['p_np'].values.flatten(), \\n                                                    test_size=TEST_RATIO, \\n                                                    random_state=42,stratify=bbbp_df['p_np'].values.flatten())\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"942a8923cbade0e5b41d9042a0e6dbe0ec62e8a1\"},\"cell_type\":\"code\",\"source\":\"def Find_Optimal_Cutoff(target, predicted):\\n    \\\"\\\"\\\" Find the optimal probability cutoff point for a classification model related to event rate\\n    Parameters\\n    ----------\\n    target : Matrix with dependent or target data, where rows are observations\\n\\n    predicted : Matrix with predicted data, where rows are observations\\n\\n    Returns\\n    -------     \\n    list type, with optimal cutoff value\\n\\n    \\\"\\\"\\\"\\n    fpr, tpr, threshold = roc_curve(target, predicted)\\n    i = np.arange(len(tpr)) \\n    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\\n    roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\\n\\n    return list(roc_t['threshold']) \",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1a53a0f3da5434b8d67e0eaa170ba988ff40499b\"},\"cell_type\":\"markdown\",\"source\":\"## Sklearn SVC Model\"},{\"metadata\":{\"trusted\":true,\"_uuid\":\"ebce39316cb4f24408cce65f832beee63e5acc33\"},\"cell_type\":\"code\",\"source\":\"parameters = {'estimator__kernel':['estimatorsigmoid', 'rbf'], 'C':[1,0.5], 'gamma':[1/nca,1/np.sqrt(nca)],'probability':[True]}\\nbbbp_svc = GridSearchCV(SVC(random_state=23,class_weight='balanced'), parameters, cv=5, scoring='roc_auc',n_jobs=-1)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"fc276f599af57a895bed9310079ebf9390691fd4\"},\"cell_type\":\"code\",\"source\":\"result = bbbp_svc.fit(X_train, y_train)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"ef4ac87da30730b47b32ddfdc132d4a91a5d15ca\"},\"cell_type\":\"code\",\"source\":\"print(result.best_estimator_)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"87579e1c257816966ad659f1a4a996ca39469cb3\"},\"cell_type\":\"code\",\"source\":\"print(result.best_score_)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"8fc2574652c60ab2a512cd8500f1469d432ef94d\"},\"cell_type\":\"code\",\"source\":\"pred = bbbp_svc.predict_proba(X_train)\\npred = pred[:,1]\\npred_svc_t = np.copy(pred)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"0792d4600043ae16dabdbdb891b41f7ec93e898b\"},\"cell_type\":\"code\",\"source\":\"threshold = Find_Optimal_Cutoff(y_train, pred)\\nprint(threshold)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"55a73929d04744f6ec2073f101a6e245ffc63b71\"},\"cell_type\":\"code\",\"source\":\"pred = bbbp_svc.predict(X_test)\\nroc_auc_score(y_test,pred)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"5c175e43bfd8f6a45127e5a9fbe3e8c91b5ef2d7\"},\"cell_type\":\"code\",\"source\":\"pred = bbbp_svc.predict_proba(X_test)\\nroc_auc_score(y_test,pred[:,1])\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"54faab5ebe30cc95b58307461570ca112d3703af\"},\"cell_type\":\"code\",\"source\":\"pred = pred[:,1]\\npred_svc = np.copy(pred)\\npred[pred<threshold] = 0\\npred[pred>=threshold] = 1\\nsvc_score = roc_auc_score(y_test,pred)\\nprint(svc_score)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"849a3a4344604c537dccdac0aaa476f8da2a5355\"},\"cell_type\":\"code\",\"source\":\"y = np.array(bbbp_descriptors_df.loc[23].values).reshape(1, -1)\\nresult = bbbp_svc.predict(y)\\nprob = bbbp_svc.predict_proba(y)\\nprint(result)\\nprint(prob)\\nprint(int(prob[:,1]>=threshold))\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"061496363ecdc73b4e8e61e7df17d693eb8b2335\"},\"cell_type\":\"markdown\",\"source\":\"## Keras Neural Network Model\"},{\"metadata\":{\"trusted\":true,\"_uuid\":\"7113e32add705b2e9f04c4376417baf588d98fad\"},\"cell_type\":\"code\",\"source\":\"bbbp_model = Sequential()\\nbbbp_model.add(Dense(128, input_dim=bbbp_descriptors_df.shape[1], \\n                     kernel_initializer='he_uniform'))\\nbbbp_model.add(BatchNormalization())\\nbbbp_model.add(Activation('tanh'))\\nbbbp_model.add(Dropout(rate=DROPRATE))\\nbbbp_model.add(Dense(64,kernel_initializer='he_uniform'))\\nbbbp_model.add(BatchNormalization())\\nbbbp_model.add(Activation('tanh'))\\nbbbp_model.add(Dropout(rate=DROPRATE))\\nbbbp_model.add(Dense(32,kernel_initializer='he_uniform'))\\nbbbp_model.add(BatchNormalization())\\nbbbp_model.add(Activation('tanh'))\\nbbbp_model.add(Dropout(rate=DROPRATE))\\nbbbp_model.add(Dense(1,kernel_initializer='he_uniform',activation='sigmoid'))\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"083f44c640f5d65d0c04125c748d1cd598bc13d5\"},\"cell_type\":\"code\",\"source\":\"bbbp_model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"31218b2bc94430d042033e8d260fe5ffad27f2d4\"},\"cell_type\":\"code\",\"source\":\"checkpoint = ModelCheckpoint('bbbp_model.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min')\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"ad06bd823a5c8b79da5579a51bd040afa01e37e3\"},\"cell_type\":\"code\",\"source\":\"unique_classes = np.unique(bbbp_df['p_np'].values.flatten())\\nclass_weights = class_weight.compute_class_weight('balanced',unique_classes,\\n                                                  bbbp_df['p_np'].values.flatten())\\nclass_weights = {unique_classes[0]:class_weights[0],unique_classes[1]:class_weights[1]}\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"23b5a0ff88474ca73e3c2e4b08cf252bec50b0c6\"},\"cell_type\":\"code\",\"source\":\"hist = bbbp_model.fit(X_train, y_train, \\n                      validation_split=VAL_RATIO,epochs=EP, batch_size=BATCH_SIZE, \\n                      class_weight=class_weights ,callbacks=[checkpoint])\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"f4ea5b8eb164011d1c69979361ee225496776df2\"},\"cell_type\":\"code\",\"source\":\"plt.ylim(0., 1.0)\\nplt.plot(hist.epoch, hist.history[\\\"loss\\\"], label=\\\"Train loss\\\")\\nplt.plot(hist.epoch, hist.history[\\\"val_loss\\\"], label=\\\"Valid loss\\\")\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"5f7724a332be6a9f671c75c55a8f67662de22529\"},\"cell_type\":\"code\",\"source\":\"bbbp_model.load_weights('bbbp_model.h5')\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"e92a97b1fafb1bcd88bb1fc0644f97bcfd73aea2\"},\"cell_type\":\"code\",\"source\":\"pred = bbbp_model.predict(X_train)\\npred_nn_t = np.copy(pred)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"5b76005aebc297af507ca7eced58ef02d2fffefc\"},\"cell_type\":\"code\",\"source\":\"threshold = Find_Optimal_Cutoff(y_train, pred)\\nprint(threshold)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"a13372a9a8d6cc6d5d511644a94d8b24cd9a6373\"},\"cell_type\":\"code\",\"source\":\"pred = bbbp_model.predict(X_test)\\npred_nn = np.copy(pred)\\nroc_auc_score(y_test,pred)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"05307f256bbad438a57d4dea3bad9c459c5d871f\"},\"cell_type\":\"code\",\"source\":\"pred[pred<threshold] = 0\\npred[pred>=threshold] = 1\\nnn_score = roc_auc_score(y_test,pred)\\nprint(nn_score)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"c800fee17a54598b552554f277ddaebd92cb15a4\"},\"cell_type\":\"code\",\"source\":\"prob = bbbp_model.predict(y)\\nprint(prob)\\nprint(int(prob>=threshold))\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"07a5dcffc37d749e7b11d3811b7dfcaee6c31a85\"},\"cell_type\":\"markdown\",\"source\":\"## Gradient Boosting of Keras Model with SVC\"},{\"metadata\":{\"trusted\":true,\"_uuid\":\"6dcd13e47350f3459a3bda94bd7254a47c925029\"},\"cell_type\":\"code\",\"source\":\"inp = bbbp_model.input\\nout = bbbp_model.layers[-2].output\\nbbbp_model_gb = Model(inp, out)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"835deec8e5b71888165f0e55f304267b3dd5953e\"},\"cell_type\":\"code\",\"source\":\"X_train = bbbp_model_gb.predict(X_train)\\nX_test = bbbp_model_gb.predict(X_test)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"2622e6161c8f4b7636da3a83709bf214d20ed60a\"},\"cell_type\":\"code\",\"source\":\"data = np.concatenate((X_train,X_test),axis=0)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"6f3a07d1cd36413ef3daf0bf233340787ac2bf4d\"},\"cell_type\":\"code\",\"source\":\"bbbp_scaler2 = StandardScaler()\\nbbbp_scaler2.fit(data)\\nX_train = bbbp_scaler2.transform(X_train)\\nX_test = bbbp_scaler2.transform(X_test)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"ee7ab0ac794f22b9d8eac8f5cb916ba233a25a7e\"},\"cell_type\":\"code\",\"source\":\"data = np.concatenate((X_train,X_test),axis=0)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"1ed210631129732b7809cca2488b4dd4005c100a\"},\"cell_type\":\"code\",\"source\":\"nca = NCA2\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"1b716cac43c839dc894db3506d817b76b2dfeb46\"},\"cell_type\":\"code\",\"source\":\"bbbp_transformer2 = KernelPCA(n_components=nca, kernel='rbf', n_jobs=-1)\\nbbbp_transformer2.fit(data)\\nX_train = bbbp_transformer2.transform(X_train)\\nX_test = bbbp_transformer2.transform(X_test)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"b8b1384488506c67177966d02d64cd84f9e4ed50\"},\"cell_type\":\"code\",\"source\":\"nca = X_train.shape[1]\\nparameters = {'kernel':['sigmoid', 'rbf'], 'C':[1,0.5], 'gamma':[1/nca,1/np.sqrt(nca)],'probability':[True]}\\nbbbp_svc_gb = GridSearchCV(SVC(random_state=23,class_weight='balanced'), parameters, cv=5, scoring='roc_auc',n_jobs=-1)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"ee40cbad1cccd9f0f356626e74489193ff70a2b1\"},\"cell_type\":\"code\",\"source\":\"result = bbbp_svc_gb.fit(X_train, y_train)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"2f4dc8f80444d1a3deef0e2fb11818321eb43666\"},\"cell_type\":\"code\",\"source\":\"print(result.best_estimator_)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"a9c82fc5293fd10a5f084e7bc2a8ee549050ce03\"},\"cell_type\":\"code\",\"source\":\"print(result.best_score_)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"7fad4c9c08b81ff8b4472349664aa0d3446c79e8\"},\"cell_type\":\"code\",\"source\":\"pred = bbbp_svc_gb.predict_proba(X_train)\\npred = pred[:,1]\\npred_svc_gb_t = np.copy(pred)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"933cf4b4cb96171082713a852e0c5510af51f2e5\"},\"cell_type\":\"code\",\"source\":\"threshold = Find_Optimal_Cutoff(y_train, pred)\\nprint(threshold)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"50c0ef3adfd3cfd694c1879b52d65167c4c6963c\"},\"cell_type\":\"code\",\"source\":\"pred = bbbp_svc_gb.predict(X_test)\\nroc_auc_score(y_test,pred)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"e6bde9d66739ad860b2286f9d0652c482b086382\"},\"cell_type\":\"code\",\"source\":\"pred = bbbp_svc_gb.predict_proba(X_test)\\nroc_auc_score(y_test,pred[:,1])\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"2031f7c8e55c5c6dd8eb88b71a869af11d9bbe94\"},\"cell_type\":\"code\",\"source\":\"pred = pred[:,1]\\npred_svc_gb = np.copy(pred)\\npred[pred<threshold] = 0\\npred[pred>=threshold] = 1\\nsvc_gb_score = roc_auc_score(y_test,pred)\\nprint(svc_gb_score)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"30f57ff4e33543a4cbfd00b0fc131b4c0b67a785\"},\"cell_type\":\"code\",\"source\":\"result = bbbp_svc_gb.predict(bbbp_model_gb.predict(y))\\nprob = bbbp_svc_gb.predict_proba(bbbp_model_gb.predict(y))\\nprint(result)\\nprint(prob)\\nprint(int(prob[:,1]>=threshold))\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"96fd5dd2b70a945a81d2dc73a31d36e8d32754e6\"},\"cell_type\":\"markdown\",\"source\":\"## Gradient Boosting of Keras Model with XGBoost\"},{\"metadata\":{\"trusted\":true,\"_uuid\":\"65c0488333a18a8dbd948e2707fc365c0bedbcea\"},\"cell_type\":\"code\",\"source\":\"parameters = {'learning_rate':[0.05,0.1,0.15],'n_estimators':[75,100,125], 'max_depth':[3,4,5],\\n               'booster':['gbtree','dart'],'reg_alpha':[0.,0.1,0.05],'reg_lambda':[0.,0.1,0.5,1.]}\\n\\nbbbp_xgb_gb = GridSearchCV(XGBClassifier(random_state=32), parameters, cv=5, scoring='roc_auc',n_jobs=-1)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"1851754076a63d9e5694ffd5906c5daad272cadb\"},\"cell_type\":\"code\",\"source\":\"result = bbbp_xgb_gb.fit(X_train, y_train)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"45215de7c6371f4f18645c12568589c9c13fc7ab\"},\"cell_type\":\"code\",\"source\":\"print(result.best_estimator_)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"3f6e18a9aa3116af2a30a2e739dd4f4f5c90745b\"},\"cell_type\":\"code\",\"source\":\"print(result.best_score_)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"e3941346e7fb7b8f3c785eabee69e715e1d154b7\"},\"cell_type\":\"code\",\"source\":\"pred = bbbp_xgb_gb.predict_proba(X_train)\\npred = pred[:,1]\\npred_xgb_gb_t= np.copy(pred)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"f36f68e469090c29f390f5b0988e3b8a40045642\"},\"cell_type\":\"code\",\"source\":\"threshold = Find_Optimal_Cutoff(y_train, pred)\\nprint(threshold)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"0847851221d7ffe3a3e8af683f7edafaeccd1d81\"},\"cell_type\":\"code\",\"source\":\"pred = bbbp_xgb_gb.predict(X_test)\\nroc_auc_score(y_test,pred)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"e8db961cad19a3e7d935b652051121613f65c2e0\"},\"cell_type\":\"code\",\"source\":\"pred = bbbp_xgb_gb.predict_proba(X_test)\\nroc_auc_score(y_test,pred[:,1])\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"3830e9abad368ff844df9de7412d2735bbe9e2b3\"},\"cell_type\":\"code\",\"source\":\"pred = pred[:,1]\\npred_xgb_gb = np.copy(pred)\\npred[pred<threshold] = 0\\npred[pred>=threshold] = 1\\nxgb_gb_score = roc_auc_score(y_test,pred)\\nprint(xgb_gb_score)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"88b29eaf696313e8c5d9a0d28c160f09cd95a745\"},\"cell_type\":\"code\",\"source\":\"result = bbbp_xgb_gb.predict(bbbp_model_gb.predict(y))\\nprob = bbbp_xgb_gb.predict_proba(bbbp_model_gb.predict(y))\\nprint(result)\\nprint(prob)\\nprint(int(prob[:,1]>=threshold))\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"1d1b651af3d8a4e7f1569bde5dc899993d03d029\"},\"cell_type\":\"code\",\"source\":\"pred = (pred_svc_t+pred_nn_t.flatten()+pred_svc_gb_t+pred_xgb_gb_t)/4.\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"8b55fcc493061f5bdf7a7e8f64e6b8182c9da439\"},\"cell_type\":\"code\",\"source\":\"threshold = Find_Optimal_Cutoff(y_train, pred)\\nprint(threshold)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"1412c57c4cb7196b5c196cb7a31ff6627b808d56\"},\"cell_type\":\"code\",\"source\":\"pred = (pred_svc+pred_nn.flatten()+pred_svc_gb+pred_xgb_gb)/4.\\npred[pred<threshold] = 0\\npred[pred>=threshold] = 1\\nave_score = roc_auc_score(y_test,pred)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"f6383fe922bb87d2367ccb675c7c928d292d6cd5\"},\"cell_type\":\"markdown\",\"source\":\"## Saving models, transformer and scaler\"},{\"metadata\":{\"trusted\":true,\"_uuid\":\"9597d9beb7f6e67a2a21ffc7ca61b9279414e14d\"},\"cell_type\":\"code\",\"source\":\"with open('bbbp_svc.pkl', 'wb') as fid:\\n    pickle.dump(bbbp_svc, fid)\\nwith open('bbbp_transformer1.pkl', 'wb') as fid:\\n    pickle.dump(bbbp_transformer1, fid)\\nwith open('bbbp_transformer2.pkl', 'wb') as fid:\\n    pickle.dump(bbbp_transformer2, fid)\\nwith open('bbbp_scaler1.pkl', 'wb') as fid:\\n    pickle.dump(bbbp_scaler1, fid)\\nwith open('bbbp_scaler2.pkl', 'wb') as fid:\\n    pickle.dump(bbbp_scaler2, fid)\\nwith open('bbbp_svc_gb.pkl', 'wb') as fid:\\n    pickle.dump(bbbp_svc_gb, fid)\\nwith open('bbbp_xgb_gb.pkl', 'wb') as fid:\\n    pickle.dump(bbbp_xgb_gb, fid)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"6f583862055407aca34e59420385e7563e0b40d3\"},\"cell_type\":\"markdown\",\"source\":\"## For loading saved model\\n\\n```python\\nwith open('bbbp_svc.pkl', 'rb') as fid:\\n    bbbp_svc = pickle.load(fid)\\n ```\"},{\"metadata\":{\"_uuid\":\"6acf37c9e9e348ec6904ed9c50f6d6c10f34ad20\"},\"cell_type\":\"markdown\",\"source\":\"# Comparision of Results with MoleculeNet results\\n\\n[http://moleculenet.ai/full-results](http://)\\n\\nThe best score on the test data for the MoleculeNet models is ~72, while the best score obtained in this kernel is ~80 on the test data. \\n\\n**Note:** Dataset in Moleculenet models are split using scaffold split, while dataset in this kernel is split using Stratified split. \\n\\n**Further Reading:**\\n* https://arxiv.org/pdf/1703.00564.pdf\"},{\"metadata\":{\"trusted\":true,\"_uuid\":\"e1450e16e14d1507ade8a5bffbbe5c8721d1eedc\"},\"cell_type\":\"code\",\"source\":\"sns.set(style=\\\"whitegrid\\\")\\nax = sns.barplot(x=[svc_score,nn_score,svc_gb_score,xgb_gb_score,ave_score],\\n                 y=['SVC','NN','SVC_GB','XGB_GB','ave'])\\nax.set(xlim=(0.75, None))\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}"
     ]
    }
   ],
   "source": [
    "#Reading .ipynb file\n",
    "!cat ../input/meta-kaggle-code/0012/006/12006057.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ff3b0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-20T07:31:55.195594Z",
     "iopub.status.busy": "2023-08-20T07:31:55.195139Z",
     "iopub.status.idle": "2023-08-20T07:31:56.302872Z",
     "shell.execute_reply": "2023-08-20T07:31:56.301135Z"
    },
    "papermill": {
     "duration": 1.1207,
     "end_time": "2023-08-20T07:31:56.305976",
     "exception": false,
     "start_time": "2023-08-20T07:31:55.185276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# -*- coding: utf-8 -*-\r\n",
      "\"\"\"summarize_reviews.ipynb\r\n",
      "\r\n",
      "Automatically generated by Colaboratory.\r\n",
      "\r\n",
      "Original file is located at\r\n",
      "    https://colab.research.google.com/drive/1p2yW4sGjibyUHBJ9gkSP9NZaxC2mte-x\r\n",
      "\r\n",
      "# Summarizing Text with Amazon Reviews\r\n",
      "\r\n",
      "The objective of this project is to build a model that can create relevant summaries for reviews written about fine foods sold on Amazon. This dataset contains above 500,000 reviews, and is hosted on [Kaggle](https://www.kaggle.com/snap/amazon-fine-food-reviews).\r\n",
      "\r\n",
      "To build our model we will use a two-layered bidirectional RNN with LSTMs on the input data and two layers, each with an LSTM using bahdanau attention on the target data.\r\n",
      "\r\n",
      "The sections of this project are:\r\n",
      "- [1.Inspecting the Data](#1.-Insepcting-the-Data)\r\n",
      "- [2.Preparing the Data](#2.-Preparing-the-Data)\r\n",
      "- [3.Building the Model](#3.-Building-the-Model)\r\n",
      "- [4.Training the Model](#4.-Training-the-Model)\r\n",
      "- [5.Making Our Own Summaries](#5.-Making-Our-Own-Summaries)\r\n",
      "\r\n",
      "## Download data\r\n",
      "Amazon Reviews Data: [Reviews.csv](https://www.kaggle.com/snap/amazon-fine-food-reviews/downloads/Reviews.csv)\r\n",
      "\r\n",
      "word embeddings [numberbatch-en-17.06.txt.gz](https://conceptnet.s3.amazonaws.com/downloads/2017/numberbatch/numberbatch-en-17.06.txt.gz)\r\n",
      "after download, extract to **./model/numberbatch-en-17.06.txt**\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "import pandas as pd\r\n",
      "import numpy as np\r\n",
      "import tensorflow as tf\r\n",
      "import re\r\n",
      "from nltk.corpus import stopwords\r\n",
      "import time\r\n",
      "from tensorflow.python.layers.core import Dense\r\n",
      "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\r\n",
      "from tensorflow.python.ops import array_ops\r\n",
      "from tensorflow.python.ops import tensor_array_ops\r\n",
      "print('TensorFlow Version: {}'.format(tf.__version__))\r\n",
      "\r\n",
      "import pickle\r\n",
      "def __pickleStuff(filename, stuff):\r\n",
      "    save_stuff = open(filename, \"wb\")\r\n",
      "    pickle.dump(stuff, save_stuff)\r\n",
      "    save_stuff.close()\r\n",
      "def __loadStuff(filename):\r\n",
      "    saved_stuff = open(filename,\"rb\")\r\n",
      "    stuff = pickle.load(saved_stuff)\r\n",
      "    saved_stuff.close()\r\n",
      "    return stuff\r\n",
      "\r\n",
      "\"\"\"## Load those prepared data and skip to section \"[3. Building the Model](#3.-Building-the-Model)\"\r\n",
      "Once we have run through the \"[2.Preparing the Data](#2.-Preparing-the-Data)\" section, we should have those data, uncomment and run those lines.\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "#clean_summaries = __loadStuff(\"./data/clean_summaries.p\")\r\n",
      "#clean_texts = __loadStuff(\"./data/clean_texts.p\")\r\n",
      "\r\n",
      "#sorted_summaries = __loadStuff(\"./data/sorted_summaries.p\")\r\n",
      "#sorted_texts = __loadStuff(\"./data/sorted_texts.p\")\r\n",
      "#word_embedding_matrix = __loadStuff(\"./data/word_embedding_matrix.p\")\r\n",
      "\r\n",
      "#vocab_to_int = __loadStuff(\"./data/vocab_to_int.p\")\r\n",
      "#int_to_vocab = __loadStuff(\"./data/int_to_vocab.p\")\r\n",
      "\r\n",
      "\"\"\"## 1. Insepcting the Data\"\"\"\r\n",
      "\r\n",
      "reviews = pd.read_csv(\"../input/amazon-fine-food-reviews/Reviews.csv\")\r\n",
      "\r\n",
      "reviews.shape\r\n",
      "\r\n",
      "reviews.head()\r\n",
      "\r\n",
      "# Check for any nulls values\r\n",
      "reviews.isnull().sum()\r\n",
      "\r\n",
      "# Remove null values and unneeded features\r\n",
      "reviews = reviews.dropna()\r\n",
      "reviews = reviews.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator',\r\n",
      "                        'Score','Time'], 1)\r\n",
      "reviews = reviews.reset_index(drop=True)\r\n",
      "\r\n",
      "reviews.shape\r\n",
      "\r\n",
      "reviews.head()\r\n",
      "\r\n",
      "# Inspecting some of the reviews\r\n",
      "for i in range(5):\r\n",
      "    print(\"Review #\",i+1)\r\n",
      "    print(reviews.Summary[i])\r\n",
      "    print(reviews.Text[i])\r\n",
      "    print()\r\n",
      "\r\n",
      "\"\"\"## 2. Preparing the Data\"\"\"\r\n",
      "\r\n",
      "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\r\n",
      "contractions = { \r\n",
      "\"ain't\": \"am not\",\r\n",
      "\"aren't\": \"are not\",\r\n",
      "\"can't\": \"cannot\",\r\n",
      "\"can't've\": \"cannot have\",\r\n",
      "\"'cause\": \"because\",\r\n",
      "\"could've\": \"could have\",\r\n",
      "\"couldn't\": \"could not\",\r\n",
      "\"couldn't've\": \"could not have\",\r\n",
      "\"didn't\": \"did not\",\r\n",
      "\"doesn't\": \"does not\",\r\n",
      "\"don't\": \"do not\",\r\n",
      "\"hadn't\": \"had not\",\r\n",
      "\"hadn't've\": \"had not have\",\r\n",
      "\"hasn't\": \"has not\",\r\n",
      "\"haven't\": \"have not\",\r\n",
      "\"he'd\": \"he would\",\r\n",
      "\"he'd've\": \"he would have\",\r\n",
      "\"he'll\": \"he will\",\r\n",
      "\"he's\": \"he is\",\r\n",
      "\"how'd\": \"how did\",\r\n",
      "\"how'll\": \"how will\",\r\n",
      "\"how's\": \"how is\",\r\n",
      "\"i'd\": \"i would\",\r\n",
      "\"i'll\": \"i will\",\r\n",
      "\"i'm\": \"i am\",\r\n",
      "\"i've\": \"i have\",\r\n",
      "\"isn't\": \"is not\",\r\n",
      "\"it'd\": \"it would\",\r\n",
      "\"it'll\": \"it will\",\r\n",
      "\"it's\": \"it is\",\r\n",
      "\"let's\": \"let us\",\r\n",
      "\"ma'am\": \"madam\",\r\n",
      "\"mayn't\": \"may not\",\r\n",
      "\"might've\": \"might have\",\r\n",
      "\"mightn't\": \"might not\",\r\n",
      "\"must've\": \"must have\",\r\n",
      "\"mustn't\": \"must not\",\r\n",
      "\"needn't\": \"need not\",\r\n",
      "\"oughtn't\": \"ought not\",\r\n",
      "\"shan't\": \"shall not\",\r\n",
      "\"sha'n't\": \"shall not\",\r\n",
      "\"she'd\": \"she would\",\r\n",
      "\"she'll\": \"she will\",\r\n",
      "\"she's\": \"she is\",\r\n",
      "\"should've\": \"should have\",\r\n",
      "\"shouldn't\": \"should not\",\r\n",
      "\"that'd\": \"that would\",\r\n",
      "\"that's\": \"that is\",\r\n",
      "\"there'd\": \"there had\",\r\n",
      "\"there's\": \"there is\",\r\n",
      "\"they'd\": \"they would\",\r\n",
      "\"they'll\": \"they will\",\r\n",
      "\"they're\": \"they are\",\r\n",
      "\"they've\": \"they have\",\r\n",
      "\"wasn't\": \"was not\",\r\n",
      "\"we'd\": \"we would\",\r\n",
      "\"we'll\": \"we will\",\r\n",
      "\"we're\": \"we are\",\r\n",
      "\"we've\": \"we have\",\r\n",
      "\"weren't\": \"were not\",\r\n",
      "\"what'll\": \"what will\",\r\n",
      "\"what're\": \"what are\",\r\n",
      "\"what's\": \"what is\",\r\n",
      "\"what've\": \"what have\",\r\n",
      "\"where'd\": \"where did\",\r\n",
      "\"where's\": \"where is\",\r\n",
      "\"who'll\": \"who will\",\r\n",
      "\"who's\": \"who is\",\r\n",
      "\"won't\": \"will not\",\r\n",
      "\"wouldn't\": \"would not\",\r\n",
      "\"you'd\": \"you would\",\r\n",
      "\"you'll\": \"you will\",\r\n",
      "\"you're\": \"you are\"\r\n",
      "}\r\n",
      "\r\n",
      "def clean_text(text, remove_stopwords = True):\r\n",
      "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\r\n",
      "    \r\n",
      "    # Convert words to lower case\r\n",
      "    text = text.lower()\r\n",
      "    \r\n",
      "    # Replace contractions with their longer forms \r\n",
      "    if True:\r\n",
      "        # We are not using \"text.split()\" here\r\n",
      "        #since it is not fool proof, e.g. words followed by punctuations \"Are you kidding?I think you aren't.\"\r\n",
      "        text = re.findall(r\"[\\w']+\", text)\r\n",
      "        new_text = []\r\n",
      "        for word in text:\r\n",
      "            if word in contractions:\r\n",
      "                new_text.append(contractions[word])\r\n",
      "            else:\r\n",
      "                new_text.append(word)\r\n",
      "        text = \" \".join(new_text)\r\n",
      "    \r\n",
      "    # Format words and remove unwanted characters\r\n",
      "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)# remove links\r\n",
      "    text = re.sub(r'\\<a href', ' ', text)# remove html link tag\r\n",
      "    text = re.sub(r'&amp;', '', text) \r\n",
      "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\r\n",
      "    text = re.sub(r'<br />', ' ', text)\r\n",
      "    text = re.sub(r'\\'', ' ', text)\r\n",
      "    \r\n",
      "    # Optionally, remove stop words\r\n",
      "    if remove_stopwords:\r\n",
      "        text = text.split()\r\n",
      "        stops = set(stopwords.words(\"english\"))\r\n",
      "        text = [w for w in text if not w in stops]\r\n",
      "        text = \" \".join(text)\r\n",
      "\r\n",
      "    return text\r\n",
      "\r\n",
      "clean_text(\"That's a great movie,Can you believe it?I've.But you may not.\")\r\n",
      "\r\n",
      "\"\"\"### Clean the summaries and texts\r\n",
      "We will remove the stopwords from the texts because they do not provide much use for training our model. However, we will keep them for our summaries so that they sound more like natural phrases.\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "clean_summaries = []\r\n",
      "for summary in reviews.Summary:\r\n",
      "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\r\n",
      "print(\"Summaries are complete.\")\r\n",
      "\r\n",
      "clean_texts = []\r\n",
      "for text in reviews.Text:\r\n",
      "    clean_texts.append(clean_text(text))\r\n",
      "print(\"Texts are complete.\")\r\n",
      "\r\n",
      "# Inspect the cleaned summaries and texts to ensure they have been cleaned well\r\n",
      "for i in range(5):\r\n",
      "    print(\"Clean Review #\",i+1)\r\n",
      "    print(clean_summaries[i])\r\n",
      "    print(clean_texts[i])\r\n",
      "    print()\r\n",
      "\r\n",
      "\"\"\"### Count the number of occurrences of each word in a set of text\"\"\"\r\n",
      "\r\n",
      "def count_words(count_dict, text):\r\n",
      "    for sentence in text:\r\n",
      "        for word in sentence.split():\r\n",
      "            if word not in count_dict:\r\n",
      "                count_dict[word] = 1\r\n",
      "            else:\r\n",
      "                count_dict[word] += 1\r\n",
      "\r\n",
      "\"\"\"#### Give the function a try\"\"\"\r\n",
      "\r\n",
      "mydict = {}\r\n",
      "count_words(mydict, [\"that is a great great great dog\",\"you have a great dog\"])\r\n",
      "mydict\r\n",
      "\r\n",
      "word_counts = {}\r\n",
      "count_words(word_counts, clean_summaries)\r\n",
      "count_words(word_counts, clean_texts)\r\n",
      "print(\"Size of Vocabulary:\", len(word_counts))\r\n",
      "\r\n",
      "\"\"\"Let's see how may \"hero\" occurs in the data\"\"\"\r\n",
      "\r\n",
      "word_counts[\"hero\"]\r\n",
      "\r\n",
      "\"\"\"### Load Conceptnet Numberbatch's (CN) embeddings, similar to GloVe, but probably better \r\n",
      " (https://github.com/commonsense/conceptnet-numberbatch)\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "embeddings_index = {}\r\n",
      "with open('../input/numberbatch1706/numberbatch-en-17.06.txt/numberbatch-en-17.06.txt', encoding='utf-8') as f:\r\n",
      "    for line in f:\r\n",
      "        values = line.split(' ')\r\n",
      "        word = values[0]\r\n",
      "        embedding = np.asarray(values[1:], dtype='float32')\r\n",
      "        embeddings_index[word] = embedding\r\n",
      "\r\n",
      "print('Word embeddings:', len(embeddings_index))\r\n",
      "\r\n",
      "\"\"\"### Take a look at the CN embedding dimension\"\"\"\r\n",
      "\r\n",
      "embeddings_index[\"hero\"].shape\r\n",
      "\r\n",
      "\"\"\"### Find the number of words that are missing from CN, and are used more than our threshold.\r\n",
      "\r\n",
      "I use a **threshold** of 20, so that words not in CN can be added to our **word_embedding_matrix**, but they need to be common enough in the reviews so that the model can understand their meaning.\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "missing_words = 0\r\n",
      "threshold = 20\r\n",
      "\r\n",
      "for word, count in word_counts.items():\r\n",
      "    if count > threshold:\r\n",
      "        if word not in embeddings_index:\r\n",
      "            missing_words += 1\r\n",
      "            \r\n",
      "missing_ratio = round(missing_words/len(word_counts),4)*100\r\n",
      "            \r\n",
      "print(\"Number of words missing from CN:\", missing_words)\r\n",
      "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))\r\n",
      "\r\n",
      "\"\"\"### What are those missing words in the CN\r\n",
      "Looks mostly products' brand.\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "missing_words = []\r\n",
      "for word, count in word_counts.items():\r\n",
      "    if count > threshold and word not in embeddings_index:\r\n",
      "        missing_words.append((word,count))\r\n",
      "missing_words[:30]\r\n",
      "\r\n",
      "\"\"\"### Words to indexes, indexes to words dicts\r\n",
      "Limit the vocab that we will use to words that appear ≥ threshold or are in CN\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "#dictionary to convert words to integers\r\n",
      "vocab_to_int = {} \r\n",
      "# Index words from 0\r\n",
      "value = 0\r\n",
      "for word, count in word_counts.items():\r\n",
      "    if count >= threshold or word in embeddings_index:\r\n",
      "        vocab_to_int[word] = value\r\n",
      "        value += 1\r\n",
      "\r\n",
      "# Special tokens that will be added to our vocab\r\n",
      "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \r\n",
      "\r\n",
      "# Add codes to vocab\r\n",
      "for code in codes:\r\n",
      "    vocab_to_int[code] = len(vocab_to_int)\r\n",
      "\r\n",
      "# Dictionary to convert integers to words\r\n",
      "int_to_vocab = {}\r\n",
      "for word, value in vocab_to_int.items():\r\n",
      "    int_to_vocab[value] = word\r\n",
      "\r\n",
      "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\r\n",
      "\r\n",
      "print(\"Total number of unique words:\", len(word_counts))\r\n",
      "print(\"Number of words we will use:\", len(vocab_to_int))\r\n",
      "print(\"Percent of words we will use: {}%\".format(usage_ratio))\r\n",
      "\r\n",
      "\"\"\"### Create word embedding matrix\r\n",
      "It has shape (nb_words, embedding_dim) i.e. (59072, 300) in this case. 1st dim is word index, 2nd dim is from CN or random generated.\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "# Need to use 300 for embedding dimensions to match CN's vectors.\r\n",
      "embedding_dim = 300\r\n",
      "nb_words = len(vocab_to_int)\r\n",
      "\r\n",
      "# Create matrix with default values of zero\r\n",
      "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\r\n",
      "for word, i in vocab_to_int.items():\r\n",
      "    if word in embeddings_index:\r\n",
      "        word_embedding_matrix[i] = embeddings_index[word]\r\n",
      "    else:\r\n",
      "        # If word not in CN, create a random embedding for it\r\n",
      "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\r\n",
      "        embeddings_index[word] = new_embedding\r\n",
      "        word_embedding_matrix[i] = new_embedding\r\n",
      "\r\n",
      "# Check if value matches len(vocab_to_int)\r\n",
      "print(len(word_embedding_matrix))\r\n",
      "\r\n",
      "\"\"\"### Function to convert sentences to sequence of words indexes\r\n",
      "It also use `<UNK>` index to replace unknown words, append `<EOS>` (End of Sentence) to the sequences if eos is set True\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "def convert_to_ints(text, word_count, unk_count, eos=False):\r\n",
      "    '''Convert words in text to an integer.\r\n",
      "       If word is not in vocab_to_int, use UNK's integer.\r\n",
      "       Total the number of words and UNKs.\r\n",
      "       Add EOS token to the end of texts'''\r\n",
      "    ints = []\r\n",
      "    for sentence in text:\r\n",
      "        sentence_ints = []\r\n",
      "        for word in sentence.split():\r\n",
      "            word_count += 1\r\n",
      "            if word in vocab_to_int:\r\n",
      "                sentence_ints.append(vocab_to_int[word])\r\n",
      "            else:\r\n",
      "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\r\n",
      "                unk_count += 1\r\n",
      "        if eos:\r\n",
      "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\r\n",
      "        ints.append(sentence_ints)\r\n",
      "    return ints, word_count, unk_count\r\n",
      "\r\n",
      "\"\"\"Apply convert_to_ints to clean_summaries and clean_texts\"\"\"\r\n",
      "\r\n",
      "word_count = 0\r\n",
      "unk_count = 0\r\n",
      "\r\n",
      "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\r\n",
      "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\r\n",
      "\r\n",
      "unk_percent = round(unk_count/word_count,4)*100\r\n",
      "\r\n",
      "print(\"Total number of words in headlines:\", word_count)\r\n",
      "print(\"Total number of UNKs in headlines:\", unk_count)\r\n",
      "print(\"Percent of words that are UNK: {}%\".format(unk_percent))\r\n",
      "\r\n",
      "\"\"\"### Take a look at what the sequence looks like\r\n",
      "Each number here represents a word\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "int_summaries[:3]\r\n",
      "\r\n",
      "\"\"\"### Function to get the length of each sequence\"\"\"\r\n",
      "\r\n",
      "def create_lengths(text):\r\n",
      "    '''Create a data frame of the sentence lengths from a text'''\r\n",
      "    lengths = []\r\n",
      "    for sentence in text:\r\n",
      "        lengths.append(len(sentence))\r\n",
      "    return pd.DataFrame(lengths, columns=['counts'])\r\n",
      "\r\n",
      "create_lengths(int_summaries[:3])\r\n",
      "\r\n",
      "\"\"\"Get statistic summary of the length of summaries and texts\"\"\"\r\n",
      "\r\n",
      "lengths_summaries = create_lengths(int_summaries)\r\n",
      "lengths_texts = create_lengths(int_texts)\r\n",
      "\r\n",
      "print(\"Summaries:\")\r\n",
      "print(lengths_summaries.describe())\r\n",
      "print()\r\n",
      "print(\"Texts:\")\r\n",
      "print(lengths_texts.describe())\r\n",
      "\r\n",
      "\"\"\"### See what's the max squence length we can cover by percentile\"\"\"\r\n",
      "\r\n",
      "# Inspect the length of texts\r\n",
      "print(np.percentile(lengths_texts.counts, 89.5))\r\n",
      "print(np.percentile(lengths_texts.counts, 95))\r\n",
      "print(np.percentile(lengths_texts.counts, 99))\r\n",
      "\r\n",
      "# Inspect the length of summaries\r\n",
      "print(np.percentile(lengths_summaries.counts, 90))\r\n",
      "print(np.percentile(lengths_summaries.counts, 95))\r\n",
      "print(np.percentile(lengths_summaries.counts, 99))\r\n",
      "\r\n",
      "\"\"\"## Function to counts the number of time `<UNK>` appears in a sentence\"\"\"\r\n",
      "\r\n",
      "def unk_counter(sentence):\r\n",
      "    '''Counts the number of time UNK appears in a sentence.'''\r\n",
      "    unk_count = 0\r\n",
      "    for word in sentence:\r\n",
      "        if word == vocab_to_int[\"<UNK>\"]:\r\n",
      "            unk_count += 1\r\n",
      "    return unk_count\r\n",
      "\r\n",
      "\"\"\"**Filter** for length limit and number of `<UNK>`s\r\n",
      "\r\n",
      "**Sort** the summaries and texts by the length of the element in **texts** from shortest to longest\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "max_text_length = 83 # This will cover up to 89.5% lengthes\r\n",
      "max_summary_length = 13 # This will cover up to 99% lengthes\r\n",
      "min_length = 2\r\n",
      "unk_text_limit = 1 # text can contain up to 1 UNK word\r\n",
      "unk_summary_limit = 0 # Summary should not contain any UNK word\r\n",
      "\r\n",
      "def filter_condition(item):\r\n",
      "    int_summary = item[0]\r\n",
      "    int_text = item[1]\r\n",
      "    if(len(int_summary) >= min_length and \r\n",
      "       len(int_summary) <= max_summary_length and \r\n",
      "       len(int_text) >= min_length and \r\n",
      "       len(int_text) <= max_text_length and \r\n",
      "       unk_counter(int_summary) <= unk_summary_limit and \r\n",
      "       unk_counter(int_text) <= unk_text_limit):\r\n",
      "        return True\r\n",
      "    else:\r\n",
      "        return False\r\n",
      "\r\n",
      "int_text_summaries = list(zip(int_summaries , int_texts))\r\n",
      "int_text_summaries_filtered = list(filter(filter_condition, int_text_summaries))\r\n",
      "sorted_int_text_summaries = sorted(int_text_summaries_filtered, key=lambda item: len(item[1]))\r\n",
      "sorted_int_text_summaries = list(zip(*sorted_int_text_summaries))\r\n",
      "sorted_summaries = list(sorted_int_text_summaries[0])\r\n",
      "sorted_texts = list(sorted_int_text_summaries[1])\r\n",
      "# Delete those temporary varaibles\r\n",
      "del int_text_summaries, sorted_int_text_summaries, int_text_summaries_filtered\r\n",
      "# Compare lengths to ensure they match\r\n",
      "print(len(sorted_summaries))\r\n",
      "print(len(sorted_texts))\r\n",
      "\r\n",
      "\"\"\"### Inspect the length of text in sorted_texts\"\"\"\r\n",
      "\r\n",
      "lengths_texts = [len(text) for text in sorted_texts]\r\n",
      "lengths_texts[:20]\r\n",
      "\r\n",
      "\"\"\"## Save data for later\"\"\"\r\n",
      "\r\n",
      "#__pickleStuff(\"../output/clean_summaries.p\",clean_summaries)\r\n",
      "#__pickleStuff(\"../output/clean_texts.p\",clean_texts)\r\n",
      "\r\n",
      "#__pickleStuff(\"../output/sorted_summaries.p\",sorted_summaries)\r\n",
      "#__pickleStuff(\"../output/sorted_texts.p\",sorted_texts)\r\n",
      "#__pickleStuff(\"../output/word_embedding_matrix.p\",word_embedding_matrix)\r\n",
      "\r\n",
      "#__pickleStuff(\"../output/vocab_to_int.p\",vocab_to_int)\r\n",
      "#__pickleStuff(\"../output/int_to_vocab.p\",int_to_vocab)\r\n",
      "\r\n",
      "\"\"\"## 3. Building the Model\r\n",
      "\r\n",
      "Create palceholders for inputs to the model\r\n",
      "\r\n",
      "**summary_length** and **text_length** are the sentence lengths in a batch, and **max_summary_length** is the maximum length of a summary in a batch.\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "def model_inputs():\r\n",
      "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\r\n",
      "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\r\n",
      "    lr = tf.placeholder(tf.float32, name='learning_rate')\r\n",
      "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\r\n",
      "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\r\n",
      "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\r\n",
      "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\r\n",
      "\r\n",
      "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length\r\n",
      "\r\n",
      "\"\"\"Remove the last word id from each batch and concatenate the id of `<GO>` to the begining of each batch\"\"\"\r\n",
      "\r\n",
      "def process_encoding_input(target_data, vocab_to_int, batch_size):  \r\n",
      "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1]) # slice it to target_data[0:batch_size, 0: -1]\r\n",
      "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\r\n",
      "\r\n",
      "    return dec_input\r\n",
      "\r\n",
      "\"\"\"### Create the encoding layers\r\n",
      "\r\n",
      "bidirectional_dynamic_rnn\r\n",
      "use **tf.variable_scope** so that variables are reused with each layer\r\n",
      "\r\n",
      "parameters\r\n",
      "- **rnn_size**: The number of units in the LSTM cell\r\n",
      "- **sequence_length**: size [batch_size], containing the actual lengths for each of the sequences in the batch\r\n",
      "- **num_layers**: number of bidirectional RNN layer\r\n",
      "- **rnn_inputs**: number of bidirectional RNN layer\r\n",
      "- **keep_prob**: RNN dropout input keep probability\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\r\n",
      "    for layer in range(num_layers):\r\n",
      "        with tf.variable_scope('encoder_{}'.format(layer)):\r\n",
      "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\r\n",
      "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\r\n",
      "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \r\n",
      "                                                    input_keep_prob = keep_prob)\r\n",
      "\r\n",
      "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\r\n",
      "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\r\n",
      "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \r\n",
      "                                                    input_keep_prob = keep_prob)\r\n",
      "\r\n",
      "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \r\n",
      "                                                                    cell_bw, \r\n",
      "                                                                    rnn_inputs,\r\n",
      "                                                                    sequence_length,\r\n",
      "                                                                    dtype=tf.float32)\r\n",
      "            enc_output = tf.concat(enc_output,2)\r\n",
      "            # original code is missing this line below, that is how we connect layers \r\n",
      "            # by feeding the current layer's output to next layer's input\r\n",
      "            rnn_inputs = enc_output\r\n",
      "    return enc_output, enc_state\r\n",
      "\r\n",
      "\"\"\"### Create the training decoding layer\r\n",
      "parameters\r\n",
      "- **dec_embed_input**: output of embedding_lookup for a batch of inputs\r\n",
      "- **summary_length**: length of each padded summary sequences in batch, since padded, all lengths should be same number \r\n",
      "- **dec_cell**: the decoder RNN cells' output with attention wapper\r\n",
      "- **output_layer**: fully connected layer to apply to the RNN output\r\n",
      "- **vocab_size**: vocabulary size i.e. len(vocab_to_int)+1\r\n",
      "- **max_summary_length**: the maximum length of a summary in a batch\r\n",
      "- **batch_size**: number of input sequences in a batch\r\n",
      "\r\n",
      "Three components\r\n",
      "\r\n",
      "- **TraingHelper** reads a sequence of integers from the encoding layer.\r\n",
      "- **BasicDecoder** processes the sequence with the decoding cell, and an output layer, which is a fully connected layer. **initial_state** set to zero state.\r\n",
      "- **dynamic_decode** creates our outputs that will be used for training.\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, output_layer,\r\n",
      "                            vocab_size, max_summary_length,batch_size):\r\n",
      "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\r\n",
      "                                                        sequence_length=summary_length,\r\n",
      "                                                        time_major=False)\r\n",
      "\r\n",
      "    training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell,\r\n",
      "                                                       helper=training_helper,\r\n",
      "                                                       initial_state=dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\r\n",
      "                                                       output_layer = output_layer)\r\n",
      "\r\n",
      "    training_logits = tf.contrib.seq2seq.dynamic_decode(training_decoder,\r\n",
      "                                                           output_time_major=False,\r\n",
      "                                                           impute_finished=True,\r\n",
      "                                                           maximum_iterations=max_summary_length)\r\n",
      "    return training_logits\r\n",
      "\r\n",
      "\"\"\"### Create infer decoding layer\r\n",
      "\r\n",
      "parameters\r\n",
      "- **embeddings**: the CN's word_embedding_matrix\r\n",
      "- **start_token**: the id of `<GO>`\r\n",
      "- **end_token**: the id of `<EOS>`\r\n",
      "- **dec_cell**: the decoder RNN cells' output with attention wapper\r\n",
      "- **output_layer**: fully connected layer to apply to the RNN output\r\n",
      "- **max_summary_length**: the maximum length of a summary in a batch\r\n",
      "- **batch_size**: number of input sequences in a batch\r\n",
      "\r\n",
      "**GreedyEmbeddingHelper** argument **start_tokens**: int32 vector shaped [batch_size], the start tokens.\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, output_layer,\r\n",
      "                             max_summary_length, batch_size):\r\n",
      "    '''Create the inference logits'''\r\n",
      "    \r\n",
      "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\r\n",
      "    \r\n",
      "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\r\n",
      "                                                                start_tokens,\r\n",
      "                                                                end_token)\r\n",
      "                \r\n",
      "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\r\n",
      "                                                        inference_helper,\r\n",
      "                                                        dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\r\n",
      "                                                        output_layer)\r\n",
      "                \r\n",
      "    inference_logits = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\r\n",
      "                                                            output_time_major=False,\r\n",
      "                                                            impute_finished=True,\r\n",
      "                                                            maximum_iterations=max_summary_length)\r\n",
      "    \r\n",
      "    return inference_logits\r\n",
      "\r\n",
      "\"\"\"### Create Decoding layer\r\n",
      "3 parts: decoding cell, attention, and getting our logits.\r\n",
      "#### Decoding Cell: \r\n",
      "Just a two layer LSTM with dropout.\r\n",
      "#### Attention: \r\n",
      "Using Bhadanau, since trains faster than Luong. \r\n",
      "\r\n",
      "**AttentionWrapper** applies the attention mechanism to our decoding cell.\r\n",
      "\r\n",
      "parameters\r\n",
      "- **dec_embed_input**: output of embedding_lookup for a batch of inputs\r\n",
      "- **embeddings**: the CN's word_embedding_matrix\r\n",
      "- **enc_output**: encoder layer output, containing the forward and the backward rnn output\r\n",
      "- **enc_state**: encoder layer state, a tuple containing the forward and the backward final states of bidirectional rnn.\r\n",
      "- **vocab_size**: vocabulary size i.e. len(vocab_to_int)+1\r\n",
      "- **text_length**: the actual lengths for each of the input text sequences in the batch\r\n",
      "- **summary_length**: the actual lengths for each of the input summary sequences in the batch\r\n",
      "- **max_summary_length**: the maximum length of a summary in a batch\r\n",
      "- **rnn_size**: The number of units in the LSTM cell\r\n",
      "- **vocab_to_int**: vocab_to_int the dictionary\r\n",
      "- **keep_prob**: RNN dropout input keep probability\r\n",
      "- **batch_size**: number of input sequences in a batch\r\n",
      "- **num_layers**: number of decoder RNN layer\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "def lstm_cell(lstm_size, keep_prob):\r\n",
      "    cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\r\n",
      "    return tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = keep_prob)\r\n",
      "\r\n",
      "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length,\r\n",
      "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\r\n",
      "    '''Create the decoding cell and attention for the training and inference decoding layers'''\r\n",
      "    dec_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(num_layers)])\r\n",
      "    output_layer = Dense(vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\r\n",
      "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\r\n",
      "                                                     enc_output,\r\n",
      "                                                     text_length,\r\n",
      "                                                     normalize=False,\r\n",
      "                                                     name='BahdanauAttention')\r\n",
      "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,attn_mech,rnn_size)\r\n",
      "    with tf.variable_scope(\"decode\"):\r\n",
      "        training_logits = training_decoding_layer(dec_embed_input,summary_length,dec_cell,\r\n",
      "                                                  output_layer,\r\n",
      "                                                  vocab_size,\r\n",
      "                                                  max_summary_length,\r\n",
      "                                                  batch_size)\r\n",
      "    with tf.variable_scope(\"decode\", reuse=True):\r\n",
      "        inference_logits = inference_decoding_layer(embeddings,\r\n",
      "                                                    vocab_to_int['<GO>'],\r\n",
      "                                                    vocab_to_int['<EOS>'],\r\n",
      "                                                    dec_cell,\r\n",
      "                                                    output_layer,\r\n",
      "                                                    max_summary_length,\r\n",
      "                                                    batch_size)\r\n",
      "    return training_logits, inference_logits\r\n",
      "\r\n",
      "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \r\n",
      "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\r\n",
      "    '''Use the previous functions to create the training and inference logits'''\r\n",
      "    \r\n",
      "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\r\n",
      "    embeddings = word_embedding_matrix\r\n",
      "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\r\n",
      "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\r\n",
      "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size) #shape=(batch_size, senquence length) each seq start with index of<GO>\r\n",
      "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\r\n",
      "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \r\n",
      "                                                        embeddings,\r\n",
      "                                                        enc_output,\r\n",
      "                                                        enc_state, \r\n",
      "                                                        vocab_size, \r\n",
      "                                                        text_length, \r\n",
      "                                                        summary_length, \r\n",
      "                                                        max_summary_length,\r\n",
      "                                                        rnn_size, \r\n",
      "                                                        vocab_to_int, \r\n",
      "                                                        keep_prob, \r\n",
      "                                                        batch_size,\r\n",
      "                                                        num_layers)\r\n",
      "    return training_logits, inference_logits\r\n",
      "\r\n",
      "\"\"\"### Pad sentences for batch\r\n",
      "Pad so the actual lengths for each of the sequences in the batch have the same length.\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "def pad_sentence_batch(sentence_batch):\r\n",
      "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\r\n",
      "    max_sentence = max([len(sentence) for sentence in sentence_batch])\r\n",
      "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]\r\n",
      "\r\n",
      "\"\"\"### Function to generate batch data for training\"\"\"\r\n",
      "\r\n",
      "def get_batches(summaries, texts, batch_size):\r\n",
      "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\r\n",
      "    for batch_i in range(0, len(texts)//batch_size):\r\n",
      "        start_i = batch_i * batch_size\r\n",
      "        summaries_batch = summaries[start_i:start_i + batch_size]\r\n",
      "        texts_batch = texts[start_i:start_i + batch_size]\r\n",
      "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\r\n",
      "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\r\n",
      "        \r\n",
      "        # Need the lengths for the _lengths parameters\r\n",
      "        pad_summaries_lengths = []\r\n",
      "        for summary in pad_summaries_batch:\r\n",
      "            pad_summaries_lengths.append(len(summary))\r\n",
      "        \r\n",
      "        pad_texts_lengths = []\r\n",
      "        for text in pad_texts_batch:\r\n",
      "            pad_texts_lengths.append(len(text))\r\n",
      "        \r\n",
      "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths\r\n",
      "\r\n",
      "\"\"\"#### Just to test \"get_batches\" function\r\n",
      "Here we generate a batch with size of 5\r\n",
      "\r\n",
      "Checkout those \"59069\" they are `<PAD>`s, also all sequences' lengths are the same.\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "print(\"'<PAD>' has id: {}\".format(vocab_to_int['<PAD>']))\r\n",
      "sorted_summaries_samples = sorted_summaries[7:50]\r\n",
      "sorted_texts_samples = sorted_texts[7:50]\r\n",
      "pad_summaries_batch_samples, pad_texts_batch_samples, pad_summaries_lengths_samples, pad_texts_lengths_samples = next(get_batches(\r\n",
      "    sorted_summaries_samples, sorted_texts_samples, 5))\r\n",
      "print(\"pad summaries batch samples:\\n\\r {}\".format(pad_summaries_batch_samples))\r\n",
      "\r\n",
      "# Set the Hyperparameters\r\n",
      "epochs = 100\r\n",
      "batch_size = 64\r\n",
      "rnn_size = 256\r\n",
      "num_layers = 2\r\n",
      "learning_rate = 0.005\r\n",
      "keep_probability = 0.95\r\n",
      "\r\n",
      "\"\"\"## Build graph\"\"\"\r\n",
      "\r\n",
      "# Build the graph\r\n",
      "train_graph = tf.Graph()\r\n",
      "# Set the graph to default to ensure that it is ready for training\r\n",
      "with train_graph.as_default():\r\n",
      "    \r\n",
      "    # Load the model inputs    \r\n",
      "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\r\n",
      "\r\n",
      "    # Create the training and inference logits\r\n",
      "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\r\n",
      "                                                      targets, \r\n",
      "                                                      keep_prob,   \r\n",
      "                                                      text_length,\r\n",
      "                                                      summary_length,\r\n",
      "                                                      max_summary_length,\r\n",
      "                                                      len(vocab_to_int)+1,\r\n",
      "                                                      rnn_size, \r\n",
      "                                                      num_layers, \r\n",
      "                                                      vocab_to_int,\r\n",
      "                                                      batch_size)\r\n",
      "    \r\n",
      "    # Create tensors for the training logits and inference logits\r\n",
      "    training_logits = tf.identity(training_logits[0].rnn_output, 'logits')\r\n",
      "    inference_logits = tf.identity(inference_logits[0].sample_id, name='predictions')\r\n",
      "    \r\n",
      "    # Create the weights for sequence_loss, the sould be all True across since each batch is padded\r\n",
      "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\r\n",
      "\r\n",
      "    with tf.name_scope(\"optimization\"):\r\n",
      "        # Loss function\r\n",
      "        cost = tf.contrib.seq2seq.sequence_loss(\r\n",
      "            training_logits,\r\n",
      "            targets,\r\n",
      "            masks)\r\n",
      "\r\n",
      "        # Optimizer\r\n",
      "        optimizer = tf.train.AdamOptimizer(learning_rate)\r\n",
      "\r\n",
      "        # Gradient Clipping\r\n",
      "        gradients = optimizer.compute_gradients(cost)\r\n",
      "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\r\n",
      "        train_op = optimizer.apply_gradients(capped_gradients)\r\n",
      "print(\"Graph is built.\")\r\n",
      "graph_location = \"../output/graph\"\r\n",
      "print(graph_location)\r\n",
      "train_writer = tf.summary.FileWriter(graph_location)\r\n",
      "train_writer.add_graph(train_graph)\r\n",
      "\r\n",
      "\"\"\"## 4. Training the Model\r\n",
      "\r\n",
      "Only going to use a subset of the data to reduce the traing time for this demo.\r\n",
      "\r\n",
      "We chose not use use the start of the subset because because those are shorter sequences and we don't want to make it too easy for the model.\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "# Subset the data for training\r\n",
      "start = 200000\r\n",
      "end = start + 50000\r\n",
      "sorted_summaries_short = sorted_summaries[start:end]\r\n",
      "sorted_texts_short = sorted_texts[start:end]\r\n",
      "print(\"The shortest text length:\", len(sorted_texts_short[0]))\r\n",
      "print(\"The longest text length:\",len(sorted_texts_short[-1]))\r\n",
      "\r\n",
      "# Train the Model\r\n",
      "learning_rate_decay = 0.95\r\n",
      "min_learning_rate = 0.0005\r\n",
      "display_step = 20 # Check training loss after every 20 batches\r\n",
      "stop_early = 0 \r\n",
      "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\r\n",
      "per_epoch = 3 # Make 3 update checks per epoch\r\n",
      "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\r\n",
      "\r\n",
      "update_loss = 0 \r\n",
      "batch_loss = 0\r\n",
      "summary_update_loss = [] # Record the update losses for saving improvements in the model\r\n",
      "\r\n",
      "checkpoint = \"../output/best_model.ckpt\" \r\n",
      "with tf.Session(graph=train_graph) as sess:\r\n",
      "    sess.run(tf.global_variables_initializer())\r\n",
      "    \r\n",
      "    # If we want to continue training a previous session\r\n",
      "    #loader = tf.train.import_meta_graph(\"../output/\" + checkpoint + '.meta')\r\n",
      "    #loader.restore(sess, checkpoint)\r\n",
      "    \r\n",
      "    for epoch_i in range(1, epochs+1):\r\n",
      "        update_loss = 0\r\n",
      "        batch_loss = 0\r\n",
      "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\r\n",
      "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\r\n",
      "            start_time = time.time()\r\n",
      "            _, loss = sess.run(\r\n",
      "                [train_op, cost],\r\n",
      "                {input_data: texts_batch,\r\n",
      "                 targets: summaries_batch,\r\n",
      "                 lr: learning_rate,\r\n",
      "                 summary_length: summaries_lengths,\r\n",
      "                 text_length: texts_lengths,\r\n",
      "                 keep_prob: keep_probability})\r\n",
      "\r\n",
      "            batch_loss += loss\r\n",
      "            update_loss += loss\r\n",
      "            end_time = time.time()\r\n",
      "            batch_time = end_time - start_time\r\n",
      "\r\n",
      "            if batch_i % display_step == 0 and batch_i > 0:\r\n",
      "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\r\n",
      "                      .format(epoch_i,\r\n",
      "                              epochs, \r\n",
      "                              batch_i, \r\n",
      "                              len(sorted_texts_short) // batch_size, \r\n",
      "                              batch_loss / display_step, \r\n",
      "                              batch_time*display_step))\r\n",
      "                batch_loss = 0\r\n",
      "\r\n",
      "            if batch_i % update_check == 0 and batch_i > 0:\r\n",
      "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\r\n",
      "                summary_update_loss.append(update_loss)\r\n",
      "                \r\n",
      "                # If the update loss is at a new minimum, save the model\r\n",
      "                if update_loss <= min(summary_update_loss):\r\n",
      "                    print('New Record!') \r\n",
      "                    stop_early = 0\r\n",
      "                    saver = tf.train.Saver() \r\n",
      "                    saver.save(sess, checkpoint)\r\n",
      "\r\n",
      "                else:\r\n",
      "                    print(\"No Improvement.\")\r\n",
      "                    stop_early += 1\r\n",
      "                    if stop_early == stop:\r\n",
      "                        break\r\n",
      "                update_loss = 0\r\n",
      "            \r\n",
      "                    \r\n",
      "        # Reduce learning rate, but not below its minimum value\r\n",
      "        learning_rate *= learning_rate_decay\r\n",
      "        if learning_rate < min_learning_rate:\r\n",
      "            learning_rate = min_learning_rate\r\n",
      "        \r\n",
      "        if stop_early == stop:\r\n",
      "            print(\"Stopping Training.\")\r\n",
      "            break\r\n",
      "\r\n",
      "\"\"\"## 5. Making Our Own Summaries\r\n",
      "\r\n",
      "To see the quality of the summaries that this model can generate, you can either create your own review, or use a review from the dataset. You can set the length of the summary to a fixed value, or use a random value like I have here.\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "def text_to_seq(text):\r\n",
      "    '''Prepare the text for the model'''\r\n",
      "    \r\n",
      "    text = clean_text(text)\r\n",
      "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]\r\n",
      "\r\n",
      "\"\"\"- **input_sentences**: a list of reviews strings we are going to summarize\r\n",
      "- **generagte_summary_length**: a int or list, if a list must be same length as input_sentences\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "input_sentences=[\"The coffee tasted great and was at such a good price! I highly recommend this to everyone!\",\r\n",
      "               \"love individual oatmeal cups found years ago sam quit selling sound big lots quit selling found target expensive buy individually trilled get entire case time go anywhere need water microwave spoon know quaker flavor packets\"]\r\n",
      "generagte_summary_length =  [3,2]\r\n",
      "\r\n",
      "texts = [text_to_seq(input_sentence) for input_sentence in input_sentences]\r\n",
      "checkpoint = \"../output/best_model.ckpt\"\r\n",
      "if type(generagte_summary_length) is list:\r\n",
      "    if len(input_sentences)!=len(generagte_summary_length):\r\n",
      "        raise Exception(\"[Error] makeSummaries parameter generagte_summary_length must be same length as input_sentences or an integer\")\r\n",
      "    generagte_summary_length_list = generagte_summary_length\r\n",
      "else:\r\n",
      "    generagte_summary_length_list = [generagte_summary_length] * len(texts)\r\n",
      "loaded_graph = tf.Graph()\r\n",
      "with tf.Session(graph=loaded_graph) as sess:\r\n",
      "    # Load saved model\r\n",
      "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\r\n",
      "    loader.restore(sess, checkpoint)\r\n",
      "    input_data = loaded_graph.get_tensor_by_name('input:0')\r\n",
      "    logits = loaded_graph.get_tensor_by_name('predictions:0')\r\n",
      "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\r\n",
      "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\r\n",
      "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\r\n",
      "    #Multiply by batch_size to match the model's input parameters\r\n",
      "    for i, text in enumerate(texts):\r\n",
      "        generagte_summary_length = generagte_summary_length_list[i]\r\n",
      "        answer_logits = sess.run(logits, {input_data: [text]*batch_size, \r\n",
      "                                          summary_length: [generagte_summary_length], #summary_length: [np.random.randint(5,8)], \r\n",
      "                                          text_length: [len(text)]*batch_size,\r\n",
      "                                          keep_prob: 1.0})[0] \r\n",
      "        # Remove the padding from the summaries\r\n",
      "        pad = vocab_to_int[\"<PAD>\"] \r\n",
      "        print('- Review:\\n\\r {}'.format(input_sentences[i]))\r\n",
      "        print('- Summary:\\n\\r {}\\n\\r\\n\\r'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))\r\n",
      "\r\n",
      "\"\"\"## Summary\r\n",
      "\r\n",
      "I hope that you found this project to be rather interesting and informative. One of my main recommendations for working with this dataset and model is either use a GPU, a subset of the dataset, or plenty of time to train your model. As you might be able to expect, the model will not be able to make good predictions just by seeing many reviews, it needs so see the reviews many times to be able to understand the relationship between words and between descriptions & summaries. \r\n",
      "\r\n",
      "In short, I'm pleased with how well this model performs. After creating numerous reviews and checking those from the dataset, I can happily say that most of the generated summaries are appropriate, some of them are great, and some of them make mistakes. I'll try to improve this model and if it gets better, I'll update my GitHub.\r\n",
      "\r\n",
      "Thanks for reading!\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "#Reading .py file\n",
    "!cat ../input/meta-kaggle-code/0012/006/12006424.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e5eaa",
   "metadata": {
    "papermill": {
     "duration": 0.009492,
     "end_time": "2023-08-20T07:31:56.325208",
     "exception": false,
     "start_time": "2023-08-20T07:31:56.315716",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Count of files with extension .ipynp , .r and .py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99954935",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-20T07:31:56.347084Z",
     "iopub.status.busy": "2023-08-20T07:31:56.346641Z",
     "iopub.status.idle": "2023-08-20T07:31:56.379605Z",
     "shell.execute_reply": "2023-08-20T07:31:56.378352Z"
    },
    "papermill": {
     "duration": 0.047239,
     "end_time": "2023-08-20T07:31:56.382367",
     "exception": false,
     "start_time": "2023-08-20T07:31:56.335128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List of subfolders\n",
    "sub_fl = os.listdir(\"../input/meta-kaggle-code/\")\n",
    "len(sub_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5af3355",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-20T07:31:56.405600Z",
     "iopub.status.busy": "2023-08-20T07:31:56.405138Z",
     "iopub.status.idle": "2023-08-20T07:32:09.241672Z",
     "shell.execute_reply": "2023-08-20T07:32:09.240389Z"
    },
    "papermill": {
     "duration": 12.852494,
     "end_time": "2023-08-20T07:32:09.245078",
     "exception": false,
     "start_time": "2023-08-20T07:31:56.392584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipynb_list = glob('../input/meta-kaggle-code/0012/*/*.ipynb')\n",
    "py_list = glob('../input/meta-kaggle-code/0012/*/*.py')\n",
    "r_list = glob('../input/meta-kaggle-code/0012/*/*.r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc22d7b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-20T07:32:09.267444Z",
     "iopub.status.busy": "2023-08-20T07:32:09.266266Z",
     "iopub.status.idle": "2023-08-20T07:32:09.275379Z",
     "shell.execute_reply": "2023-08-20T07:32:09.273891Z"
    },
    "papermill": {
     "duration": 0.023015,
     "end_time": "2023-08-20T07:32:09.278108",
     "exception": false,
     "start_time": "2023-08-20T07:32:09.255093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31299, 1411, 277)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ipynb_list), len(py_list), len(r_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac991bf",
   "metadata": {
    "papermill": {
     "duration": 0.009784,
     "end_time": "2023-08-20T07:32:09.298498",
     "exception": false,
     "start_time": "2023-08-20T07:32:09.288714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**EDA on Juyter Notebook folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b355510",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-20T07:32:09.319955Z",
     "iopub.status.busy": "2023-08-20T07:32:09.319562Z",
     "iopub.status.idle": "2023-08-20T07:32:09.324723Z",
     "shell.execute_reply": "2023-08-20T07:32:09.323849Z"
    },
    "papermill": {
     "duration": 0.019265,
     "end_time": "2023-08-20T07:32:09.327529",
     "exception": false,
     "start_time": "2023-08-20T07:32:09.308264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/meta-kaggle-code/0012/437/12437691.ipynb\n"
     ]
    }
   ],
   "source": [
    "print(ipynb_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "590784fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-20T07:32:09.349520Z",
     "iopub.status.busy": "2023-08-20T07:32:09.349069Z",
     "iopub.status.idle": "2023-08-20T07:32:09.360517Z",
     "shell.execute_reply": "2023-08-20T07:32:09.359439Z"
    },
    "papermill": {
     "duration": 0.025847,
     "end_time": "2023-08-20T07:32:09.363319",
     "exception": false,
     "start_time": "2023-08-20T07:32:09.337472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Checking first ipynb file\n",
    "import json\n",
    "with open('../input/meta-kaggle-code/0012/437/12437691.ipynb','r') as i:\n",
    "    d_1 = json.load(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beb8d501",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-20T07:32:09.385386Z",
     "iopub.status.busy": "2023-08-20T07:32:09.384906Z",
     "iopub.status.idle": "2023-08-20T07:32:09.404157Z",
     "shell.execute_reply": "2023-08-20T07:32:09.403278Z"
    },
    "papermill": {
     "duration": 0.032953,
     "end_time": "2023-08-20T07:32:09.406467",
     "exception": false,
     "start_time": "2023-08-20T07:32:09.373514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cells': [{'metadata': {},\n",
       "   'cell_type': 'markdown',\n",
       "   'source': '**[Python Micro-Course Home Page](https://www.kaggle.com/learn/python)**\\n\\n---\\n'},\n",
       "  {'metadata': {},\n",
       "   'cell_type': 'markdown',\n",
       "   'source': \"These exercises accompany the tutorial on [functions and getting help](https://www.kaggle.com/colinmorris/functions-and-getting-help).\\n\\nAs before, don't forget to run the setup code below before jumping into question 1.\"},\n",
       "  {'metadata': {'_kg_hide-input': False,\n",
       "    '_kg_hide-output': True,\n",
       "    'trusted': True},\n",
       "   'cell_type': 'code',\n",
       "   'source': \"# SETUP. You don't need to worry for now about what this code does or how it works.\\nfrom learntools.core import binder; binder.bind(globals())\\nfrom learntools.python.ex2 import *\\nprint('Setup complete.')\",\n",
       "   'execution_count': 1,\n",
       "   'outputs': [{'output_type': 'stream',\n",
       "     'text': 'Setup complete.\\n',\n",
       "     'name': 'stdout'}]},\n",
       "  {'metadata': {}, 'cell_type': 'markdown', 'source': '# Exercises'},\n",
       "  {'metadata': {},\n",
       "   'cell_type': 'markdown',\n",
       "   'source': '## 1.\\n\\nComplete the body of the following function according to its docstring.\\n\\nHINT: Python has a builtin function `round`'},\n",
       "  {'metadata': {'trusted': True},\n",
       "   'cell_type': 'code',\n",
       "   'source': 'def round_to_two_places(num):\\n    \"\"\"Return the given number rounded to two decimal places. \\n    \\n    >>> round_to_two_places(3.14159)\\n    3.14\\n    \"\"\"\\n    # Replace this body with your own code.\\n    # (\"pass\" is a keyword that does literally nothing. We used it as a placeholder\\n    # because after we begin a code block, Python requires at least one line of code)\\n    return (round(num, 2))\\n\\nq1.check()',\n",
       "   'execution_count': 3,\n",
       "   'outputs': [{'output_type': 'display_data',\n",
       "     'data': {'text/plain': '<IPython.core.display.Javascript object>',\n",
       "      'application/javascript': 'parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"learnTutorialId\": 104, \"questionId\": \"1_RoundFunctionProblem\", \"learnToolsVersion\": \"0.2.13\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")'},\n",
       "     'metadata': {}},\n",
       "    {'output_type': 'display_data',\n",
       "     'data': {'text/plain': 'Correct',\n",
       "      'text/markdown': '<span style=\"color:#33cc33\">Correct</span>'},\n",
       "     'metadata': {}}]},\n",
       "  {'metadata': {'trusted': False},\n",
       "   'cell_type': 'code',\n",
       "   'source': '# Uncomment the following for a hint\\n#q1.hint()\\n# Or uncomment the following to peek at the solution\\n#q1.solution()',\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'metadata': {},\n",
       "   'cell_type': 'markdown',\n",
       "   'source': '## 2.\\nThe help for `round` says that `ndigits` (the second argument) may be negative.\\nWhat do you think will happen when it is? Try some examples in the following cell?\\n\\nCan you think of a case where this would be useful?'},\n",
       "  {'metadata': {'trusted': True},\n",
       "   'cell_type': 'code',\n",
       "   'source': '# Put your test code here\\nround(122.2222, -2)',\n",
       "   'execution_count': 7,\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'execution_count': 7,\n",
       "     'data': {'text/plain': '100.0'},\n",
       "     'metadata': {}}]},\n",
       "  {'metadata': {'trusted': True},\n",
       "   'cell_type': 'code',\n",
       "   'source': '#q2.solution()',\n",
       "   'execution_count': 6,\n",
       "   'outputs': []},\n",
       "  {'metadata': {},\n",
       "   'cell_type': 'markdown',\n",
       "   'source': \"## 3.\\n\\nIn a previous programming problem, the candy-sharing friends Alice, Bob and Carol tried to split candies evenly. For the sake of their friendship, any candies left over would be smashed. For example, if they collectively bring home 91 candies, they'll take 30 each and smash 1.\\n\\nBelow is a simple function that will calculate the number of candies to smash for *any* number of total candies.\\n\\nModify it so that it optionally takes a second argument representing the number of friends the candies are being split between. If no second argument is provided, it should assume 3 friends, as before.\\n\\nUpdate the docstring to reflect this new behaviour.\"},\n",
       "  {'metadata': {'trusted': True},\n",
       "   'cell_type': 'code',\n",
       "   'source': 'def to_smash(total_candies, number_friends = 3):\\n    \"\"\"Return the number of leftover candies that must be smashed after distributing\\n    the given number of candies evenly between 3 friends.\\n    \\n    >>> to_smash(91)\\n    1\\n    \"\"\"\\n    if number_friends > 0:\\n        return total_candies % number_friends\\n\\nq3.check()',\n",
       "   'execution_count': 15,\n",
       "   'outputs': [{'output_type': 'display_data',\n",
       "     'data': {'text/plain': '<IPython.core.display.Javascript object>',\n",
       "      'application/javascript': 'parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"learnTutorialId\": 104, \"questionId\": \"3_CandySmashingFunctionProblem\", \"learnToolsVersion\": \"0.2.13\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")'},\n",
       "     'metadata': {}},\n",
       "    {'output_type': 'display_data',\n",
       "     'data': {'text/plain': 'Correct',\n",
       "      'text/markdown': '<span style=\"color:#33cc33\">Correct</span>'},\n",
       "     'metadata': {}}]},\n",
       "  {'metadata': {'trusted': True},\n",
       "   'cell_type': 'code',\n",
       "   'source': '#q3.hint()',\n",
       "   'execution_count': 14,\n",
       "   'outputs': []},\n",
       "  {'metadata': {'trusted': False},\n",
       "   'cell_type': 'code',\n",
       "   'source': '#q3.solution()',\n",
       "   'execution_count': None,\n",
       "   'outputs': []},\n",
       "  {'metadata': {},\n",
       "   'cell_type': 'markdown',\n",
       "   'source': \"## 4.\\n\\nIt may not be fun, but reading and understanding error messages will be an important part of your Python career.\\n\\nEach code cell below contains some commented-out buggy code. For each cell...\\n\\n1. Read the code and predict what you think will happen when it's run.\\n2. Then uncomment the code and run it to see what happens. (**Tip**: In the kernel editor, you can highlight several lines and press `ctrl`+`/` to toggle commenting.)\\n3. Fix the code (so that it accomplishes its intended purpose without throwing an exception)\\n\\n<!-- TODO: should this be autochecked? Delta is probably pretty small. -->\"},\n",
       "  {'metadata': {'trusted': True},\n",
       "   'cell_type': 'code',\n",
       "   'source': ' round_to_two_places(9.9999)',\n",
       "   'execution_count': 17,\n",
       "   'outputs': [{'output_type': 'execute_result',\n",
       "     'execution_count': 17,\n",
       "     'data': {'text/plain': '10.0'},\n",
       "     'metadata': {}}]},\n",
       "  {'metadata': {'trusted': True},\n",
       "   'cell_type': 'code',\n",
       "   'source': 'x = -10\\ny = 5\\n# # Which of the two variables above has the smallest absolute value?\\nsmallest_abs = min(abs(x), abs(y))',\n",
       "   'execution_count': 23,\n",
       "   'outputs': []},\n",
       "  {'metadata': {'trusted': True},\n",
       "   'cell_type': 'code',\n",
       "   'source': 'def f(x):\\n    y = abs(x)\\n    return y\\n\\nprint(f(5))',\n",
       "   'execution_count': 26,\n",
       "   'outputs': [{'output_type': 'stream', 'text': '5\\n', 'name': 'stdout'}]},\n",
       "  {'metadata': {},\n",
       "   'cell_type': 'markdown',\n",
       "   'source': '## 5. <span title=\"A bit spicy\" style=\"color: darkgreen \">🌶️</span>\\n\\nFor this question, we\\'ll be using two functions imported from Python\\'s `time` module.\\n\\nThe [time](https://docs.python.org/3/library/time.html#time.time) function returns the number of seconds that have passed since the Epoch (aka [Unix time](https://en.wikipedia.org/wiki/Unix_time)). \\n\\n<!-- We\\'ve provided a function called `seconds_since_epoch` which returns the number of seconds that have passed since the Epoch (aka [Unix time](https://en.wikipedia.org/wiki/Unix_time)). -->\\n\\nTry it out below. Each time you run it, you should get a slightly larger number.'},\n",
       "  {'metadata': {'trusted': True},\n",
       "   'cell_type': 'code',\n",
       "   'source': '# Importing the function \\'time\\' from the module of the same name. \\n# (We\\'ll discuss imports in more depth later)\\nfrom time import time\\nt = time()\\nprint(t, \"seconds since the Epoch\")',\n",
       "   'execution_count': 30,\n",
       "   'outputs': [{'output_type': 'stream',\n",
       "     'text': '1554302999.2191966 seconds since the Epoch\\n',\n",
       "     'name': 'stdout'}]},\n",
       "  {'metadata': {},\n",
       "   'cell_type': 'markdown',\n",
       "   'source': \"We'll also be using a function called [sleep](https://docs.python.org/3/library/time.html#time.sleep), which makes us wait some number of seconds while it does nothing particular. (Sounds useful, right?)\\n\\nYou can see it in action by running the cell below:\"},\n",
       "  {'metadata': {'trusted': True},\n",
       "   'cell_type': 'code',\n",
       "   'source': 'from time import sleep\\nduration = 5\\nprint(\"Getting sleepy. See you in\", duration, \"seconds\")\\nsleep(duration)\\nprint(\"I\\'m back. What did I miss?\")',\n",
       "   'execution_count': 31,\n",
       "   'outputs': [{'output_type': 'stream',\n",
       "     'text': \"Getting sleepy. See you in 5 seconds\\nI'm back. What did I miss?\\n\",\n",
       "     'name': 'stdout'}]},\n",
       "  {'metadata': {},\n",
       "   'cell_type': 'markdown',\n",
       "   'source': 'With the help of these functions, complete the function `time_call` below according to its docstring.\\n\\n<!-- (The sleep function will be useful for testing here since we have a pretty good idea of what something like `time_call(sleep, 1)` should return.) -->'},\n",
       "  {'metadata': {'trusted': True},\n",
       "   'cell_type': 'code',\n",
       "   'source': 'from time import time\\ndef time_call(fn, arg):\\n    \"\"\"Return the amount of time the given function takes (in seconds) when called with the given argument.\\n    \"\"\"\\n    t = time()\\n    return ',\n",
       "   'execution_count': 33,\n",
       "   'outputs': []},\n",
       "  {'metadata': {},\n",
       "   'cell_type': 'markdown',\n",
       "   'source': 'How would you verify that `time_call` is working correctly? Think about it, and then check the answer with the `solution` function below`.'},\n",
       "  {'metadata': {'trusted': True},\n",
       "   'cell_type': 'code',\n",
       "   'source': 'q5.hint()\\nq5.solution()',\n",
       "   'execution_count': 35,\n",
       "   'outputs': [{'output_type': 'display_data',\n",
       "     'data': {'text/plain': '<IPython.core.display.Javascript object>',\n",
       "      'application/javascript': 'parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 2, \"learnTutorialId\": 104, \"questionId\": \"5_TimeCallProblem\", \"learnToolsVersion\": \"0.2.13\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")'},\n",
       "     'metadata': {}},\n",
       "    {'output_type': 'display_data',\n",
       "     'data': {'text/plain': \"Hint: You'll need to call `time()` before and after calling the input function in order to measure its running time. The `sleep` function will be very useful for testing.\",\n",
       "      'text/markdown': '<span style=\"color:#3366cc\">Hint:</span> You\\'ll need to call `time()` before and after calling the input function in order to measure its running time. The `sleep` function will be very useful for testing.'},\n",
       "     'metadata': {}},\n",
       "    {'output_type': 'display_data',\n",
       "     'data': {'text/plain': '<IPython.core.display.Javascript object>',\n",
       "      'application/javascript': 'parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"learnTutorialId\": 104, \"questionId\": \"5_TimeCallProblem\", \"learnToolsVersion\": \"0.2.13\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")'},\n",
       "     'metadata': {}},\n",
       "    {'output_type': 'display_data',\n",
       "     'data': {'text/plain': 'Solution: Example function body:\\n```python\\nt0 = time()\\nfn(arg)\\nt1 = time()\\nelapsed = t1 - t0\\nreturn elapsed\\n```\\nTo test your function, you can run something like `time_call(sleep, 2)` and make sure its return value is close to 2. ',\n",
       "      'text/markdown': '<span style=\"color:#33cc99\">Solution:</span> Example function body:\\n```python\\nt0 = time()\\nfn(arg)\\nt1 = time()\\nelapsed = t1 - t0\\nreturn elapsed\\n```\\nTo test your function, you can run something like `time_call(sleep, 2)` and make sure its return value is close to 2. \\n'},\n",
       "     'metadata': {}}]},\n",
       "  {'metadata': {},\n",
       "   'cell_type': 'markdown',\n",
       "   'source': '## 6. <span title=\"A bit spicy\" style=\"color: darkgreen \">🌶️</span>\\n\\n*Note: this question depends on a working solution to the previous question.*\\n\\nComplete the function below according to its docstring.'},\n",
       "  {'metadata': {'trusted': True},\n",
       "   'cell_type': 'code',\n",
       "   'source': 'def slowest_call(fn, arg1, arg2, arg3):\\n    \"\"\"Return the amount of time taken by the slowest of the following function\\n    calls: fn(arg1), fn(arg2), fn(arg3)\\n    \"\"\"\\n    return (max(time_call(fn, arg1), time_call(fn, arg2), time_call(fn, arg3)))',\n",
       "   'execution_count': 48,\n",
       "   'outputs': []},\n",
       "  {'metadata': {'trusted': True},\n",
       "   'cell_type': 'code',\n",
       "   'source': '#q6.hint()',\n",
       "   'execution_count': 39,\n",
       "   'outputs': []},\n",
       "  {'metadata': {'trusted': True},\n",
       "   'cell_type': 'code',\n",
       "   'source': '#q6.solution()',\n",
       "   'execution_count': 50,\n",
       "   'outputs': []},\n",
       "  {'metadata': {},\n",
       "   'cell_type': 'markdown',\n",
       "   'source': '# Keep Going\\n\\nYou are ready for **[booleans and conditionals](https://www.kaggle.com/colinmorris/booleans-and-conditionals).**\\n'},\n",
       "  {'metadata': {},\n",
       "   'cell_type': 'markdown',\n",
       "   'source': '---\\n**[Python Micro-Course Home Page](https://www.kaggle.com/learn/python)**\\n\\n'}],\n",
       " 'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
       "   'language': 'python',\n",
       "   'name': 'python3'},\n",
       "  'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
       "   'file_extension': '.py',\n",
       "   'mimetype': 'text/x-python',\n",
       "   'name': 'python',\n",
       "   'nbconvert_exporter': 'python',\n",
       "   'pygments_lexer': 'ipython3',\n",
       "   'version': '3.6.5'},\n",
       "  'learntools_metadata': {'lesson_index': 1, 'type': 'exercise'}},\n",
       " 'nbformat': 4,\n",
       " 'nbformat_minor': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4c761a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-20T07:32:09.432565Z",
     "iopub.status.busy": "2023-08-20T07:32:09.431701Z",
     "iopub.status.idle": "2023-08-20T07:32:09.439237Z",
     "shell.execute_reply": "2023-08-20T07:32:09.437832Z"
    },
    "papermill": {
     "duration": 0.023866,
     "end_time": "2023-08-20T07:32:09.442058",
     "exception": false,
     "start_time": "2023-08-20T07:32:09.418192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cells', 'metadata', 'nbformat', 'nbformat_minor'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_1.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626dea29",
   "metadata": {
    "papermill": {
     "duration": 0.010493,
     "end_time": "2023-08-20T07:32:09.463924",
     "exception": false,
     "start_time": "2023-08-20T07:32:09.453431",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "*Assuming All the notebooks are having same keys*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "632cfcc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-20T07:32:09.487671Z",
     "iopub.status.busy": "2023-08-20T07:32:09.487239Z",
     "iopub.status.idle": "2023-08-20T07:32:09.495395Z",
     "shell.execute_reply": "2023-08-20T07:32:09.494196Z"
    },
    "papermill": {
     "duration": 0.022846,
     "end_time": "2023-08-20T07:32:09.497737",
     "exception": false,
     "start_time": "2023-08-20T07:32:09.474891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metadata': {'trusted': True},\n",
       " 'cell_type': 'code',\n",
       " 'source': 'def round_to_two_places(num):\\n    \"\"\"Return the given number rounded to two decimal places. \\n    \\n    >>> round_to_two_places(3.14159)\\n    3.14\\n    \"\"\"\\n    # Replace this body with your own code.\\n    # (\"pass\" is a keyword that does literally nothing. We used it as a placeholder\\n    # because after we begin a code block, Python requires at least one line of code)\\n    return (round(num, 2))\\n\\nq1.check()',\n",
       " 'execution_count': 3,\n",
       " 'outputs': [{'output_type': 'display_data',\n",
       "   'data': {'text/plain': '<IPython.core.display.Javascript object>',\n",
       "    'application/javascript': 'parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"learnTutorialId\": 104, \"questionId\": \"1_RoundFunctionProblem\", \"learnToolsVersion\": \"0.2.13\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")'},\n",
       "   'metadata': {}},\n",
       "  {'output_type': 'display_data',\n",
       "   'data': {'text/plain': 'Correct',\n",
       "    'text/markdown': '<span style=\"color:#33cc33\">Correct</span>'},\n",
       "   'metadata': {}}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d_1['cells']\n",
    "d_1['cells'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e656a30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-20T07:32:09.521611Z",
     "iopub.status.busy": "2023-08-20T07:32:09.521176Z",
     "iopub.status.idle": "2023-08-20T07:32:09.528477Z",
     "shell.execute_reply": "2023-08-20T07:32:09.526492Z"
    },
    "papermill": {
     "duration": 0.022596,
     "end_time": "2023-08-20T07:32:09.531268",
     "exception": false,
     "start_time": "2023-08-20T07:32:09.508672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['metadata', 'cell_type', 'source'])\n",
      "dict_keys(['metadata', 'cell_type', 'source'])\n",
      "dict_keys(['metadata', 'cell_type', 'source', 'execution_count', 'outputs'])\n",
      "dict_keys(['metadata', 'cell_type', 'source'])\n",
      "dict_keys(['metadata', 'cell_type', 'source'])\n",
      "dict_keys(['metadata', 'cell_type', 'source', 'execution_count', 'outputs'])\n",
      "dict_keys(['metadata', 'cell_type', 'source', 'execution_count', 'outputs'])\n",
      "dict_keys(['metadata', 'cell_type', 'source'])\n",
      "dict_keys(['metadata', 'cell_type', 'source', 'execution_count', 'outputs'])\n",
      "dict_keys(['metadata', 'cell_type', 'source', 'execution_count', 'outputs'])\n",
      "dict_keys(['metadata', 'cell_type', 'source'])\n",
      "dict_keys(['metadata', 'cell_type', 'source', 'execution_count', 'outputs'])\n",
      "dict_keys(['metadata', 'cell_type', 'source', 'execution_count', 'outputs'])\n",
      "dict_keys(['metadata', 'cell_type', 'source', 'execution_count', 'outputs'])\n",
      "dict_keys(['metadata', 'cell_type', 'source'])\n",
      "dict_keys(['metadata', 'cell_type', 'source', 'execution_count', 'outputs'])\n",
      "dict_keys(['metadata', 'cell_type', 'source', 'execution_count', 'outputs'])\n",
      "dict_keys(['metadata', 'cell_type', 'source', 'execution_count', 'outputs'])\n",
      "dict_keys(['metadata', 'cell_type', 'source'])\n",
      "dict_keys(['metadata', 'cell_type', 'source', 'execution_count', 'outputs'])\n",
      "dict_keys(['metadata', 'cell_type', 'source'])\n",
      "dict_keys(['metadata', 'cell_type', 'source', 'execution_count', 'outputs'])\n",
      "dict_keys(['metadata', 'cell_type', 'source'])\n",
      "dict_keys(['metadata', 'cell_type', 'source', 'execution_count', 'outputs'])\n",
      "dict_keys(['metadata', 'cell_type', 'source'])\n",
      "dict_keys(['metadata', 'cell_type', 'source', 'execution_count', 'outputs'])\n",
      "dict_keys(['metadata', 'cell_type', 'source'])\n",
      "dict_keys(['metadata', 'cell_type', 'source', 'execution_count', 'outputs'])\n",
      "dict_keys(['metadata', 'cell_type', 'source', 'execution_count', 'outputs'])\n",
      "dict_keys(['metadata', 'cell_type', 'source', 'execution_count', 'outputs'])\n",
      "dict_keys(['metadata', 'cell_type', 'source'])\n",
      "dict_keys(['metadata', 'cell_type', 'source'])\n"
     ]
    }
   ],
   "source": [
    "#Checking subkeys in cells\n",
    "for cell in d_1['cells']:\n",
    "    print(cell.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ad96666",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-20T07:32:09.556485Z",
     "iopub.status.busy": "2023-08-20T07:32:09.555501Z",
     "iopub.status.idle": "2023-08-20T07:32:09.564319Z",
     "shell.execute_reply": "2023-08-20T07:32:09.562725Z"
    },
    "papermill": {
     "duration": 0.02416,
     "end_time": "2023-08-20T07:32:09.567039",
     "exception": false,
     "start_time": "2023-08-20T07:32:09.542879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernelspec': {'display_name': 'Python 3',\n",
       "  'language': 'python',\n",
       "  'name': 'python3'},\n",
       " 'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
       "  'file_extension': '.py',\n",
       "  'mimetype': 'text/x-python',\n",
       "  'name': 'python',\n",
       "  'nbconvert_exporter': 'python',\n",
       "  'pygments_lexer': 'ipython3',\n",
       "  'version': '3.6.5'},\n",
       " 'learntools_metadata': {'lesson_index': 1, 'type': 'exercise'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking Metadata\n",
    "d_1['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36829371",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-20T07:32:09.592565Z",
     "iopub.status.busy": "2023-08-20T07:32:09.591254Z",
     "iopub.status.idle": "2023-08-20T07:32:09.599147Z",
     "shell.execute_reply": "2023-08-20T07:32:09.597834Z"
    },
    "papermill": {
     "duration": 0.023372,
     "end_time": "2023-08-20T07:32:09.602055",
     "exception": false,
     "start_time": "2023-08-20T07:32:09.578683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['kernelspec', 'language_info', 'learntools_metadata'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_1['metadata'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aea96cc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-20T07:32:09.627262Z",
     "iopub.status.busy": "2023-08-20T07:32:09.626802Z",
     "iopub.status.idle": "2023-08-20T07:32:09.634016Z",
     "shell.execute_reply": "2023-08-20T07:32:09.632786Z"
    },
    "papermill": {
     "duration": 0.022796,
     "end_time": "2023-08-20T07:32:09.636502",
     "exception": false,
     "start_time": "2023-08-20T07:32:09.613706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) [GCC 12.3.0]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(d_1['metadata']['kernelspec'])\n",
    "# print(d_1['metadata']['language_info'])\n",
    "# print(d_1['metadata']['learntools_metadata'])\n",
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f8579bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-20T07:32:09.661816Z",
     "iopub.status.busy": "2023-08-20T07:32:09.661429Z",
     "iopub.status.idle": "2023-08-20T07:32:09.667595Z",
     "shell.execute_reply": "2023-08-20T07:32:09.666288Z"
    },
    "papermill": {
     "duration": 0.021977,
     "end_time": "2023-08-20T07:32:09.670248",
     "exception": false,
     "start_time": "2023-08-20T07:32:09.648271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(d_1['nbformat'])\n",
    "print(d_1['nbformat_minor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20c2d66",
   "metadata": {
    "papermill": {
     "duration": 0.011428,
     "end_time": "2023-08-20T07:32:09.693370",
     "exception": false,
     "start_time": "2023-08-20T07:32:09.681942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**OBERVATIONS**\n",
    "While Going over ipynb files noticed following keys:\n",
    "1. Cells :- metadata, source, cell_type\n",
    "    source - execution_count, outputs\n",
    "2. metadata :- kernelspec, language_info\n",
    "3. nbformat \n",
    "4. nbformat_minor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e792dac",
   "metadata": {
    "papermill": {
     "duration": 0.011311,
     "end_time": "2023-08-20T07:32:09.716487",
     "exception": false,
     "start_time": "2023-08-20T07:32:09.705176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30.007179,
   "end_time": "2023-08-20T07:32:10.651260",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-20T07:31:40.644081",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
